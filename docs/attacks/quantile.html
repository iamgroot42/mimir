<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>mimir.attacks.quantile API documentation</title>
<meta name="description" content="Implementation of the attack proposed in &#39;Scalable Membership Inference Attacks via Quantile Regression&#39;
https://arxiv.org/pdf/2307.03694.pdf">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>mimir.attacks.quantile</code></h1>
</header>
<section id="section-intro">
<p>Implementation of the attack proposed in 'Scalable Membership Inference Attacks via Quantile Regression'
<a href="https://arxiv.org/pdf/2307.03694.pdf">https://arxiv.org/pdf/2307.03694.pdf</a></p>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="mimir.attacks.quantile.CustomTrainer"><code class="flex name class">
<span>class <span class="ident">CustomTrainer</span></span>
<span>(</span><span>alpha_fpr, **kwargs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CustomTrainer(Trainer):
    def __init__(
        self,
        alpha_fpr,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.alpha_fpr = alpha_fpr

    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.pop(&#34;labels&#34;)
        # forward pass
        outputs = model(**inputs)
        logits = outputs.get(&#34;logits&#34;)
        loss = ch.mean(
            ch.max(
                self.alpha_fpr * (logits - labels),
                (1 - self.alpha_fpr) * (labels - logits),
            )
        )
        return (loss, outputs) if return_outputs else loss</code></pre>
</details>
<div class="desc"><p>Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for ðŸ¤— Transformers.</p>
<h2 id="args">Args</h2>
<p>model ([<code>PreTrainedModel</code>] or <code>torch.nn.Module</code>, <em>optional</em>):
The model to train, evaluate or use for predictions. If not provided, a <code>model_init</code> must be passed.</p>
<pre><code>&lt;Tip&gt;

[`Trainer`] is optimized to work with the [`PreTrainedModel`] provided by the library. You can still use
your own models defined as &lt;code&gt;torch.nn.Module&lt;/code&gt; as long as they work the same way as the ðŸ¤— Transformers
models.

&lt;/Tip&gt;
</code></pre>
<p>args ([<code>TrainingArguments</code>], <em>optional</em>):
The arguments to tweak for training. Will default to a basic instance of [<code>TrainingArguments</code>] with the
<code>output_dir</code> set to a directory named <em>tmp_trainer</em> in the current directory if not provided.
data_collator (<code>DataCollator</code>, <em>optional</em>):
The function to use to form a batch from a list of elements of <code>train_dataset</code> or <code>eval_dataset</code>. Will
default to [<code>default_data_collator</code>] if no <code>processing_class</code> is provided, an instance of
[<code>DataCollatorWithPadding</code>] otherwise if the processing_class is a feature extractor or tokenizer.
train_dataset (Union[<code>torch.utils.data.Dataset</code>, <code>torch.utils.data.IterableDataset</code>, <code>datasets.Dataset</code>], <em>optional</em>):
The dataset to use for training. If it is a [<code>~datasets.Dataset</code>], columns not accepted by the
<code>model.forward()</code> method are automatically removed.</p>
<pre><code>Note that if it's a &lt;code&gt;torch.utils.data.IterableDataset&lt;/code&gt; with some randomization and you are training in a
distributed fashion, your iterable dataset should either use a internal attribute &lt;code&gt;generator&lt;/code&gt; that is a
&lt;code&gt;torch.Generator&lt;/code&gt; for the randomization that must be identical on all processes (and the Trainer will
manually set the seed of this &lt;code&gt;generator&lt;/code&gt; at each epoch) or have a &lt;code&gt;set\_epoch()&lt;/code&gt; method that internally
sets the seed of the RNGs used.
</code></pre>
<p>eval_dataset (Union[<code>torch.utils.data.Dataset</code>, dict[str, <code>torch.utils.data.Dataset</code>, <code>datasets.Dataset</code>]), <em>optional</em>):
The dataset to use for evaluation. If it is a [<code>~datasets.Dataset</code>], columns not accepted by the
<code>model.forward()</code> method are automatically removed. If it is a dictionary, it will evaluate on each
dataset prepending the dictionary key to the metric name.
processing_class (<code>PreTrainedTokenizerBase</code> or <code>BaseImageProcessor</code> or <code>FeatureExtractionMixin</code> or <code>ProcessorMixin</code>, <em>optional</em>):
Processing class used to process the data. If provided, will be used to automatically process the inputs
for the model, and it will be saved along the model to make it easier to rerun an interrupted training or
reuse the fine-tuned model.
This supersedes the <code>tokenizer</code> argument, which is now deprecated.
model_init (<code>Callable[[], PreTrainedModel]</code>, <em>optional</em>):
A function that instantiates the model to be used. If provided, each call to [<code>~Trainer.train</code>] will start
from a new instance of the model as given by this function.</p>
<pre><code>The function may have zero argument, or a single one containing the optuna/Ray Tune/SigOpt trial object, to
be able to choose different architectures according to hyper parameters (such as layer count, sizes of
inner layers, dropout probabilities etc).
</code></pre>
<p>compute_loss_func (<code>Callable</code>, <em>optional</em>):
A function that accepts the raw model outputs, labels, and the number of items in the entire accumulated
batch (batch_size * gradient_accumulation_steps) and returns the loss. For example, see the default <a href="https://github.com/huggingface/transformers/blob/052e652d6d53c2b26ffde87e039b723949a53493/src/transformers/trainer.py#L3618">loss function</a> used by [<code>Trainer</code>].
compute_metrics (<code>Callable[[EvalPrediction], Dict]</code>, <em>optional</em>):
The function that will be used to compute metrics at evaluation. Must take a [<code>EvalPrediction</code>] and return
a dictionary string to metric values. <em>Note</em> When passing TrainingArgs with <code>batch_eval_metrics</code> set to
<code>True</code>, your compute_metrics function must take a boolean <code>compute_result</code> argument. This will be triggered
after the last eval batch to signal that the function needs to calculate and return the global summary
statistics rather than accumulating the batch-level statistics
callbacks (List of [<code>TrainerCallback</code>], <em>optional</em>):
A list of callbacks to customize the training loop. Will add those to the list of default callbacks
detailed in <a href="callback">here</a>.</p>
<pre><code>If you want to remove one of the default callbacks used, use the [`Trainer.remove_callback`] method.
</code></pre>
<p>optimizers (<code>tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]</code>, <em>optional</em>, defaults to <code>(None, None)</code>):
A tuple containing the optimizer and the scheduler to use. Will default to an instance of [<code>AdamW</code>] on your
model and a scheduler given by [<code>get_linear_schedule_with_warmup</code>] controlled by <code>args</code>.
optimizer_cls_and_kwargs (<code>tuple[Type[torch.optim.Optimizer], dict[str, Any]]</code>, <em>optional</em>):
A tuple containing the optimizer class and keyword arguments to use.
Overrides <code>optim</code> and <code>optim_args</code> in <code>args</code>. Incompatible with the <code>optimizers</code> argument.</p>
<pre><code>Unlike &lt;code&gt;optimizers&lt;/code&gt;, this argument avoids the need to place model parameters on the correct devices before initializing the Trainer.
</code></pre>
<p>preprocess_logits_for_metrics (<code>Callable[[torch.Tensor, torch.Tensor], torch.Tensor]</code>, <em>optional</em>):
A function that preprocess the logits right before caching them at each evaluation step. Must take two
tensors, the logits and the labels, and return the logits once processed as desired. The modifications made
by this function will be reflected in the predictions received by <code>compute_metrics</code>.</p>
<pre><code>Note that the labels (second parameter) will be &lt;code&gt;None&lt;/code&gt; if the dataset does not have them.
</code></pre>
<p>Important attributes:</p>
<pre><code>- **model** -- Always points to the core model. If using a transformers model, it will be a [`PreTrainedModel`]
  subclass.
- **model_wrapped** -- Always points to the most external model in case one or more other modules wrap the
  original model. This is the model that should be used for the forward pass. For example, under &lt;code&gt;DeepSpeed&lt;/code&gt;,
  the inner model is wrapped in &lt;code&gt;DeepSpeed&lt;/code&gt; and then again in &lt;code&gt;torch.nn.DistributedDataParallel&lt;/code&gt;. If the inner
  model hasn't been wrapped, then &lt;code&gt;self.model\_wrapped&lt;/code&gt; is the same as &lt;code&gt;self.model&lt;/code&gt;.
- **is_model_parallel** -- Whether or not a model has been switched to a model parallel mode (different from
  data parallelism, this means some of the model layers are split on different GPUs).
- **place_model_on_device** -- Whether or not to automatically place the model on the device - it will be set
  to &lt;code&gt;False&lt;/code&gt; if model parallel or deepspeed is used, or if the default
  &lt;code&gt;TrainingArguments.place\_model\_on\_device&lt;/code&gt; is overridden to return &lt;code&gt;False&lt;/code&gt; .
- **is_in_train** -- Whether or not a model is currently running &lt;code&gt;train&lt;/code&gt; (e.g. when &lt;code&gt;evaluate&lt;/code&gt; is called while
  in &lt;code&gt;train&lt;/code&gt;)
</code></pre></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>transformers.trainer.Trainer</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="mimir.attacks.quantile.CustomTrainer.compute_loss"><code class="name flex">
<span>def <span class="ident">compute_loss</span></span>(<span>self, model, inputs, return_outputs=False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_loss(self, model, inputs, return_outputs=False):
    labels = inputs.pop(&#34;labels&#34;)
    # forward pass
    outputs = model(**inputs)
    logits = outputs.get(&#34;logits&#34;)
    loss = ch.mean(
        ch.max(
            self.alpha_fpr * (logits - labels),
            (1 - self.alpha_fpr) * (labels - logits),
        )
    )
    return (loss, outputs) if return_outputs else loss</code></pre>
</details>
<div class="desc"><p>How the loss is computed by Trainer. By default, all models return the loss in the first element.</p>
<h2 id="args">Args</h2>
<p>model (<code>nn.Module</code>):
The model to compute the loss for.
inputs (<code>dict[str, Union[torch.Tensor, Any]]</code>):
The input data for the model.
return_outputs (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether to return the model outputs along with the loss.
num_items_in_batch (Optional[torch.Tensor], <em>optional</em>):
The number of items in the batch. If num_items_in_batch is not passed,</p>
<h2 id="returns">Returns</h2>
<p>The loss of the model along with its output if return_outputs was set to True
Subclass and override for custom behavior. If you are not using <code>num_items_in_batch</code> when computing your loss,
make sure to overwrite <code>self.model_accepts_loss_kwargs</code> to <code>False</code>. Otherwise, the loss calculationg might be slightly inacurate when performing gradient accumulation.</p></div>
</dd>
</dl>
</dd>
<dt id="mimir.attacks.quantile.QuantileAttack"><code class="flex name class">
<span>class <span class="ident">QuantileAttack</span></span>
<span>(</span><span>config,<br>model:Â <a title="mimir.models.Model" href="../models.html#mimir.models.Model">Model</a>,<br>alpha:Â float)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class QuantileAttack(Attack):
    &#34;&#34;&#34;
    Implementation of the attack proposed in &#39;Scalable Membership Inference Attacks via Quantile Regression&#39;
    https://arxiv.org/pdf/2307.03694.pdf
    &#34;&#34;&#34;

    def __init__(self, config, model: Model, alpha: float):
        &#34;&#34;&#34;
        alpha (float): Desired FPR
        &#34;&#34;&#34;
        ref_model = QuantileReferenceModel(
            config, name=&#34;Sreevishnu/funnel-transformer-small-imdb&#34;
        )
        super().__init__(self, config, model, ref_model)
        self.alpha = alpha

    def _train_quantile_model(self, dataset):
        def tokenize_function(examples):
            return self.ref_model.tokenizer(
                examples[&#34;text&#34;], padding=&#34;max_length&#34;, truncation=True
            )

        tokenized_dataset = dataset.map(tokenize_function, batched=True)
        training_args = TrainingArguments(
            output_dir=&#34;quantile_ref_model&#34;,
            evaluation_strategy=&#34;epoch&#34;,
            num_train_epochs=1,
        )

        def compute_metrics(eval_pred):
            predictions, labels = eval_pred
            rmse = mean_squared_error(labels, predictions, squared=False)
            return {&#34;rmse&#34;: rmse}

        trainer = CustomTrainer(
            alpha_fpr=self.alpha,
            model=self.ref_model.model,
            args=training_args,
            train_dataset=tokenized_dataset,
            eval_dataset=tokenized_dataset,
            compute_metrics=compute_metrics,
        )
        # Train quantile model
        trainer.train()

    def prepare(self, known_non_members):
        &#34;&#34;&#34;
        Step 1: Use non-member dataset, collect confidence scores for correct label.
        Step 2: Train a quantile regression model that takes X as input and predicts quantile. Use pinball loss
        Step 3: Test by checking if member: score is higher than output of quantile regression model.
        &#34;&#34;&#34;

        # Step 1: Use non-member dataset, collect confidence scores for correct label.
        # Get likelihood scores from target model for known_non_members
        # Note that these non-members should be different from the ones in testing
        scores = [self.target_model.get_ll(x) for x in known_non_members]
        # Construct a dataset out of this to be used in Huggingface, with
        # &#34;text&#34; containing the actual data, and &#34;labels&#34; containing the scores
        dataset = Dataset.from_dict({&#34;text&#34;: known_non_members, &#34;labels&#34;: scores})

        # Step 2: Train a quantile regression model that takes X as input and predicts quantile. Use pinball loss
        self._train_quantile_model(dataset)

    def attack(self, document, **kwargs):
        # Step 3: Test by checking if member: score is higher than output of quantile regression model.

        # Get likelihood score from target model for doc
        ll = self.target_model.get_ll(document)

        # Return ll - quantile_model(doc)
        tokenized = self.ref_model.tokenizer(document, return_tensors=&#34;pt&#34;)
        # Shift items in the dictionary to the correct device
        tokenized = {k: v.to(self.ref_model.model.device, non_blocking=True) for k, v in tokenized.items()}
        quantile_score = self.ref_model.model(**tokenized)
        print(quantile_score)
        quantile_score = quantile_score.logits.item()

        # We want higher score to be non-member
        return quantile_score - ll</code></pre>
</details>
<div class="desc"><p>Implementation of the attack proposed in 'Scalable Membership Inference Attacks via Quantile Regression'
<a href="https://arxiv.org/pdf/2307.03694.pdf">https://arxiv.org/pdf/2307.03694.pdf</a></p>
<p>alpha (float): Desired FPR</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mimir.attacks.all_attacks.Attack" href="all_attacks.html#mimir.attacks.all_attacks.Attack">Attack</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="mimir.attacks.quantile.QuantileAttack.prepare"><code class="name flex">
<span>def <span class="ident">prepare</span></span>(<span>self, known_non_members)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare(self, known_non_members):
    &#34;&#34;&#34;
    Step 1: Use non-member dataset, collect confidence scores for correct label.
    Step 2: Train a quantile regression model that takes X as input and predicts quantile. Use pinball loss
    Step 3: Test by checking if member: score is higher than output of quantile regression model.
    &#34;&#34;&#34;

    # Step 1: Use non-member dataset, collect confidence scores for correct label.
    # Get likelihood scores from target model for known_non_members
    # Note that these non-members should be different from the ones in testing
    scores = [self.target_model.get_ll(x) for x in known_non_members]
    # Construct a dataset out of this to be used in Huggingface, with
    # &#34;text&#34; containing the actual data, and &#34;labels&#34; containing the scores
    dataset = Dataset.from_dict({&#34;text&#34;: known_non_members, &#34;labels&#34;: scores})

    # Step 2: Train a quantile regression model that takes X as input and predicts quantile. Use pinball loss
    self._train_quantile_model(dataset)</code></pre>
</details>
<div class="desc"><p>Step 1: Use non-member dataset, collect confidence scores for correct label.
Step 2: Train a quantile regression model that takes X as input and predicts quantile. Use pinball loss
Step 3: Test by checking if member: score is higher than output of quantile regression model.</p></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mimir.attacks.all_attacks.Attack" href="all_attacks.html#mimir.attacks.all_attacks.Attack">Attack</a></b></code>:
<ul class="hlist">
<li><code><a title="mimir.attacks.all_attacks.Attack.attack" href="all_attacks.html#mimir.attacks.all_attacks.Attack.attack">attack</a></code></li>
<li><code><a title="mimir.attacks.all_attacks.Attack.load" href="all_attacks.html#mimir.attacks.all_attacks.Attack.load">load</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<header>
<a class="homelink" rel="home" title="MIMIR Home" href="https://iamgroot42.github.io/mimir/">
<img src="https://raw.githubusercontent.com/iamgroot42/mimir/8ed6886fb6df7a72f2f0f398688f48b68c5f48b0/assets/logo.png" alt="MIMIR">
</a>
</header>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="mimir.attacks" href="index.html">mimir.attacks</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="mimir.attacks.quantile.CustomTrainer" href="#mimir.attacks.quantile.CustomTrainer">CustomTrainer</a></code></h4>
<ul class="">
<li><code><a title="mimir.attacks.quantile.CustomTrainer.compute_loss" href="#mimir.attacks.quantile.CustomTrainer.compute_loss">compute_loss</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mimir.attacks.quantile.QuantileAttack" href="#mimir.attacks.quantile.QuantileAttack">QuantileAttack</a></code></h4>
<ul class="">
<li><code><a title="mimir.attacks.quantile.QuantileAttack.prepare" href="#mimir.attacks.quantile.QuantileAttack.prepare">prepare</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
