<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>mimir.models API documentation</title>
<meta name="description" content="Model definitions, with basic helper functions. Supports any model as long as it supports the functions specified in Model." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>mimir.models</code></h1>
</header>
<section id="section-intro">
<p>Model definitions, with basic helper functions. Supports any model as long as it supports the functions specified in Model.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
    Model definitions, with basic helper functions. Supports any model as long as it supports the functions specified in Model.
&#34;&#34;&#34;
import torch
import torch.nn as nn
import openai
from typing import List
import numpy as np
import transformers
import time
from collections import defaultdict
from multiprocessing.pool import ThreadPool
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from hf_olmo import *

from mimir.config import ExperimentConfig
from mimir.custom_datasets import SEPARATOR
from mimir.data_utils import drop_last_word


class Model(nn.Module):
    &#34;&#34;&#34;
        Base class (for LLMs).
    &#34;&#34;&#34;
    def __init__(self, config: ExperimentConfig, **kwargs):
        super().__init__()
        self.model = None # Set by child class
        self.tokenizer = None # Set by child class
        self.config = config
        self.device = None
        self.device_map = None
        self.name = None
        self.kwargs = kwargs
        self.cache_dir = self.config.env_config.cache_dir

    def to(self, device):
        &#34;&#34;&#34;
            Shift model to a particular device.
        &#34;&#34;&#34;
        self.model.to(device, non_blocking=True)

    def load(self):
        &#34;&#34;&#34;
            Load model onto GPU (and compile, if requested) if not already loaded with device map.
        &#34;&#34;&#34;
        if not self.device_map:
            start = time.time()
            try:
                self.model.cpu()
            except NameError:
                pass
            if self.config.openai_config is None:
                self.model.to(self.device, non_blocking=True)
            if self.config.env_config.compile:
                torch.compile(self.model)
            print(f&#39;DONE ({time.time() - start:.2f}s)&#39;)

    def unload(self):
        &#34;&#34;&#34;
            Unload model from GPU
        &#34;&#34;&#34;
        start = time.time()
        try:
            self.model.cpu()
        except NameError:
            pass
        print(f&#39;DONE ({time.time() - start:.2f}s)&#39;)

    def get_probabilities(self,
                          text: str,
                          tokens: np.ndarray = None,
                          no_grads: bool = True):
        &#34;&#34;&#34;
            Get the probabilities or log-softmaxed logits for a text under the current model.
            Args:
                text (str): The input text for which to calculate probabilities.
                tokens (numpy.ndarray, optional): An optional array of token ids. If provided, these tokens
                are used instead of tokenizing the input text. Defaults to None.

            Raises:
                ValueError: If the device or name attributes of the instance are not set.

            Returns:
                list: A list of probabilities.
        &#34;&#34;&#34;
        with torch.set_grad_enabled(not no_grads):
            if self.device is None or self.name is None:
                raise ValueError(&#34;Please set self.device and self.name in child class&#34;)

            if tokens is not None:
                labels = torch.from_numpy(tokens.astype(np.int64)).type(torch.LongTensor)
                if labels.shape[0] != 1:
                    # expand first dimension
                    labels = labels.unsqueeze(0)
            else:
                tokenized = self.tokenizer(
                    text, return_tensors=&#34;pt&#34;)
                labels = tokenized.input_ids

            all_prob = []
            for i in range(0, labels.size(1), self.stride):
                begin_loc = max(i + self.stride - self.max_length, 0)
                end_loc = min(i + self.stride, labels.size(1))
                trg_len = end_loc - i  # may be different from stride on last loop
                input_ids = labels[:, begin_loc:end_loc].to(self.device)
                target_ids = input_ids.clone()
                target_ids[:, :-trg_len] = -100

                logits = self.model(input_ids, labels=target_ids).logits
                if no_grads:
                    logits = logits.cpu()
                shift_logits = logits[..., :-1, :].contiguous()
                probabilities = torch.nn.functional.log_softmax(shift_logits, dim=-1)
                shift_labels = target_ids[..., 1:]
                if no_grads:
                    shift_labels = shift_labels.cpu()
                shift_labels = shift_labels.contiguous()
                labels_processed = shift_labels[0]

                del input_ids
                del target_ids

                for i, token_id in enumerate(labels_processed):
                    if token_id != -100:
                        probability = probabilities[0, i, token_id]
                        if no_grads:
                            probability = probability.item()
                        all_prob.append(probability)
            # Should be equal to # of tokens - 1 to account for shift
            assert len(all_prob) == labels.size(1) - 1

        if not no_grads:
            all_prob = torch.stack(all_prob)

        return all_prob

    @torch.no_grad()
    def get_ll(self,
               text: str,
               tokens: np.ndarray=None,
               probs = None):
        &#34;&#34;&#34;
            Get the log likelihood of each text under the base_model.

            Args:
                text (str): The input text for which to calculate the log likelihood.
                tokens (numpy.ndarray, optional): An optional array of token ids. If provided, these tokens
                are used instead of tokenizing the input text. Defaults to None.
                probs (list, optional): An optional list of probabilities. If provided, these probabilities
                are used instead of calling the `get_probabilities` method. Defaults to None.
        &#34;&#34;&#34;
        all_prob = probs if probs is not None else self.get_probabilities(text, tokens=tokens)
        return -np.mean(all_prob)

    def load_base_model_and_tokenizer(self, model_kwargs):
        &#34;&#34;&#34;
            Load the base model and tokenizer for a given model name.
        &#34;&#34;&#34;
        if self.device is None or self.name is None:
            raise ValueError(&#34;Please set self.device and self.name in child class&#34;)

        if self.config.openai_config is None:
            print(f&#39;Loading BASE model {self.name}...&#39;)
            device_map = self.device_map # if self.device_map else &#39;cpu&#39;
            if &#34;silo&#34; in self.name or &#34;balanced&#34; in self.name:
                from utils.transformers.model import OpenLMforCausalLM
                model = OpenLMforCausalLM.from_pretrained(
                    self.name, **model_kwargs, device_map=self.device, cache_dir=self.cache_dir)
                # Extract the model from the model wrapper so we dont need to call model.model
            elif &#34;llama&#34; in self.name or &#34;alpaca&#34; in self.name:
                # TODO: This should be smth specified in config in case user has
                # llama is too big, gotta use device map
                model = transformers.AutoModelForCausalLM.from_pretrained(self.name, **model_kwargs, device_map=&#34;balanced_low_0&#34;, cache_dir=self.cache_dir)
                self.device = &#39;cuda:1&#39;
            elif &#34;stablelm&#34; in self.name.lower():  # models requiring custom code
                model = transformers.AutoModelForCausalLM.from_pretrained(
                    self.name, **model_kwargs, trust_remote_code=True, device_map=device_map, cache_dir=self.cache_dir)
            elif &#34;olmo&#34; in self.name.lower():
                model = transformers.AutoModelForCausalLM.from_pretrained(
                    self.name, **model_kwargs, trust_remote_code=True, cache_dir=self.cache_dir)
            else:
                model = transformers.AutoModelForCausalLM.from_pretrained(
                    self.name, **model_kwargs, device_map=device_map, cache_dir=self.cache_dir)
        else:
            model = None

        optional_tok_kwargs = {}
        if &#34;facebook/opt-&#34; in self.name:
            print(&#34;Using non-fast tokenizer for OPT&#34;)
            optional_tok_kwargs[&#39;fast&#39;] = False
        if self.config.dataset_member in [&#39;pubmed&#39;] or self.config.dataset_nonmember in [&#39;pubmed&#39;]:
            optional_tok_kwargs[&#39;padding_side&#39;] = &#39;left&#39;
            self.pad_token = self.tokenizer.eos_token_id
        if &#34;silo&#34; in self.name or &#34;balanced&#34; in self.name:
            tokenizer = transformers.GPTNeoXTokenizerFast.from_pretrained(
                &#34;EleutherAI/gpt-neox-20b&#34;, **optional_tok_kwargs, cache_dir=self.cache_dir)
        elif &#34;datablations&#34; in self.name:
            tokenizer = transformers.AutoTokenizer.from_pretrained(
                &#34;gpt2&#34;, **optional_tok_kwargs, cache_dir=self.cache_dir)
        elif &#34;llama&#34; in self.name or &#34;alpaca&#34; in self.name:
            tokenizer = transformers.LlamaTokenizer.from_pretrained(
                self.name, **optional_tok_kwargs, cache_dir=self.cache_dir)
        elif &#34;pubmedgpt&#34; in self.name:
            tokenizer = transformers.AutoTokenizer.from_pretrained(
                &#34;stanford-crfm/BioMedLM&#34;, **optional_tok_kwargs, cache_dir=self.cache_dir)
        else:
            tokenizer = transformers.AutoTokenizer.from_pretrained(
                self.name, **optional_tok_kwargs, cache_dir=self.cache_dir,
                trust_remote_code=True if &#34;olmo&#34; in self.name.lower() else False)
        tokenizer.add_special_tokens({&#39;pad_token&#39;: &#39;[PAD]&#39;})

        return model, tokenizer

    def load_model_properties(self):
        &#34;&#34;&#34;
            Load model properties, such as max length and stride.
        &#34;&#34;&#34;
        # TODO: getting max_length of input could be more generic
        if &#34;silo&#34; in self.name or &#34;balanced&#34; in self.name:
            self.max_length = self.model.model.seq_len
        elif hasattr(self.model.config, &#39;max_position_embeddings&#39;):
            self.max_length = self.model.config.max_position_embeddings
        elif hasattr(self.model.config, &#39;n_positions&#39;):
            self.max_length = self.model.config.n_positions
        else:
            # Default window size
            self.max_length = 1024
        self.stride = self.max_length // 2


class ReferenceModel(Model):
    &#34;&#34;&#34;
        Wrapper for reference model
    &#34;&#34;&#34;
    def __init__(self, config: ExperimentConfig, name: str):
        super().__init__(config)
        self.device = self.config.env_config.device_aux
        self.name = name
        base_model_kwargs = {&#39;revision&#39;: &#39;main&#39;}
        if &#39;gpt-j&#39; in self.name or &#39;neox&#39; in self.name or &#39;llama&#39; in self.name or &#39;alpaca&#39; in self.name:
            base_model_kwargs.update(dict(torch_dtype=torch.float16))
        if &#39;gpt-j&#39; in self.name:
            base_model_kwargs.update(dict(revision=&#39;float16&#39;))
        if &#39;:&#39; in self.name:
            print(&#34;Applying ref model revision&#34;)
            # Allow them to provide revisions as part of model name, then parse accordingly
            split = self.name.split(&#39;:&#39;)
            self.name = split[0]
            base_model_kwargs.update(dict(revision=split[-1]))
        self.model, self.tokenizer = self.load_base_model_and_tokenizer(
            model_kwargs=base_model_kwargs)
        self.load_model_properties()

    def load(self):
        &#34;&#34;&#34;
        Load reference model noto GPU(s)
        &#34;&#34;&#34;
        if &#34;llama&#34; not in self.name and &#34;alpaca&#34; not in self.name:
            super().load()

    def unload(self):
        &#34;&#34;&#34;
        Unload reference model from GPU(s)
        &#34;&#34;&#34;
        if &#34;llama&#34; not in self.name and &#34;alpaca&#34; not in self.name:
            super().unload()


class QuantileReferenceModel(Model):
    &#34;&#34;&#34;
        Wrapper for referenc model, specifically used for quantile regression
    &#34;&#34;&#34;
    def __init__(self, config: ExperimentConfig, name: str):
        super().__init__(config)
        self.device = self.config.env_config.device_aux
        self.name = name
        self.tokenizer = AutoTokenizer.from_pretrained(
            name, use_fast=False)
        self.model = AutoModelForSequenceClassification.from_pretrained(
            name,
            num_labels=2,
            max_position_embeddings=1024)
        # Modify model&#39;s last linear layer to have only 1 output
        self.model.classifier.linear_out = nn.Linear(self.model.classifier.linear_out.in_features, 1)
        self.load_model_properties()


class LanguageModel(Model):
    &#34;&#34;&#34;
        Generic LM- used most often for target model
    &#34;&#34;&#34;
    def __init__(self, config: ExperimentConfig, **kwargs):
        super().__init__(config, **kwargs)
        self.device = self.config.env_config.device
        self.device_map = self.config.env_config.device_map
        # Use provided name (if provided)
        # Relevant for scoring-model scenario
        self.name = self.kwargs.get(&#39;name&#39;, self.config.base_model)

        base_model_kwargs = {}
        if config.revision:
            base_model_kwargs.update(dict(revision=config.revision))
        if &#39;gpt-j&#39; in self.name or &#39;neox&#39; in self.name:
            base_model_kwargs.update(dict(torch_dtype=torch.float16))
        if &#39;gpt-j&#39; in self.name:
            base_model_kwargs.update(dict(revision=&#39;float16&#39;))
        self.model, self.tokenizer = self.load_base_model_and_tokenizer(
            model_kwargs=base_model_kwargs)
        self.load_model_properties()

    @torch.no_grad()
    def get_ref(self, text: str, ref_model: ReferenceModel, tokens=None, probs=None):
        &#34;&#34;&#34;
            Compute the loss of a given text calibrated against the text&#39;s loss under a reference model -- MIA baseline
        &#34;&#34;&#34;
        lls = self.get_ll(text, tokens=tokens, probs=probs)
        lls_ref = ref_model.get_ll(text)

        return lls - lls_ref

    @torch.no_grad()
    def get_rank(self, text: str, log: bool=False):
        &#34;&#34;&#34;
            Get the average rank of each observed token sorted by model likelihood
        &#34;&#34;&#34;
        openai_config = self.config.openai_config
        assert openai_config is None, &#34;get_rank not implemented for OpenAI models&#34;

        tokenized = self.tokenizer(text, return_tensors=&#34;pt&#34;).to(self.device)
        logits = self.model(**tokenized).logits[:,:-1]
        labels = tokenized.input_ids[:,1:]

        # get rank of each label token in the model&#39;s likelihood ordering
        matches = (logits.argsort(-1, descending=True) == labels.unsqueeze(-1)).nonzero()

        assert matches.shape[1] == 3, f&#34;Expected 3 dimensions in matches tensor, got {matches.shape}&#34;

        ranks, timesteps = matches[:,-1], matches[:,-2]

        # make sure we got exactly one match for each timestep in the sequence
        assert (timesteps == torch.arange(len(timesteps)).to(timesteps.device)).all(), &#34;Expected one match per timestep&#34;

        ranks = ranks.float() + 1 # convert to 1-indexed rank
        if log:
            ranks = torch.log(ranks)

        return ranks.float().mean().item()

    # TODO extend for longer sequences
    @torch.no_grad()
    def get_lls(self, texts: List[str], batch_size: int = 6):
        #return [self.get_ll(text) for text in texts] # -np.mean([self.get_ll(text) for text in texts])
        # tokenized = self.tokenizer(texts, return_tensors=&#34;pt&#34;, padding=True)
        # labels = tokenized.input_ids
        total_size = len(texts)
        losses = []
        for i in range(0, total_size, batch_size):
            # Delegate batches and tokenize
            batch = texts[i:i+batch_size]
            tokenized = self.tokenizer(batch, return_tensors=&#34;pt&#34;, padding=True, return_attention_mask=True)
            label_batch = tokenized.input_ids
            
            # # mask out padding tokens
            attention_mask = tokenized.attention_mask
            assert attention_mask.size() == label_batch.size()

            needs_sliding = label_batch.size(1) &gt; self.max_length // 2
            if not needs_sliding:
                label_batch = label_batch.to(self.device)
                attention_mask = attention_mask.to(self.device)

            # Collect token probabilities per sample in batch
            all_prob = defaultdict(list)
            for i in range(0, label_batch.size(1), self.stride):
                begin_loc = max(i + self.stride - self.max_length, 0)
                end_loc = min(i + self.stride, label_batch.size(1))
                trg_len = end_loc - i  # may be different from stride on last loop
                input_ids = label_batch[:, begin_loc:end_loc]
                mask = attention_mask[:, begin_loc:end_loc]
                if needs_sliding:
                    input_ids = input_ids.to(self.device)
                    mask = mask.to(self.device)
                    
                target_ids = input_ids.clone()
                # Don&#39;t count padded tokens or tokens that already have computed probabilities
                target_ids[:, :-trg_len] = -100
                # target_ids[attention_mask == 0] = -100
                
                logits = self.model(input_ids, labels=target_ids, attention_mask=mask).logits.cpu()
                target_ids = target_ids.cpu()
                shift_logits = logits[..., :-1, :].contiguous()
                probabilities = torch.nn.functional.log_softmax(shift_logits, dim=-1)
                shift_labels = target_ids[..., 1:].contiguous()

                for i, sample in enumerate(shift_labels):
                    for j, token_id in enumerate(sample):
                        if token_id != -100 and token_id != self.tokenizer.pad_token_id:
                            probability = probabilities[i, j, token_id].item()
                            all_prob[i].append(probability)

                del input_ids
                del mask
            
            # average over each sample to get losses
            batch_losses = [-np.mean(all_prob[idx]) for idx in range(label_batch.size(0))]
            # print(batch_losses)
            losses.extend(batch_losses)
            del label_batch
            del attention_mask
        return losses #np.mean(losses)

    def sample_from_model(self, texts: List[str], **kwargs):
        &#34;&#34;&#34;
            Sample from base_model using ****only**** the first 30 tokens in each example as context
        &#34;&#34;&#34;
        min_words = kwargs.get(&#39;min_words&#39;, 55)
        max_words = kwargs.get(&#39;max_words&#39;, 200)
        prompt_tokens = kwargs.get(&#39;prompt_tokens&#39;, 30)

        # encode each text as a list of token ids
        if self.config.dataset_member == &#39;pubmed&#39;:
            texts = [t[:t.index(SEPARATOR)] for t in texts]
            all_encoded = self.tokenizer(texts, return_tensors=&#34;pt&#34;, padding=True).to(self.device, non_blocking=True)
        else:
            all_encoded = self.tokenizer(texts, return_tensors=&#34;pt&#34;, padding=True).to(self.device, non_blocking=True)
            all_encoded = {key: value[:, :prompt_tokens] for key, value in all_encoded.items()}

        decoded = [&#39;&#39; for _ in range(len(texts))]

        # sample from the model until we get a sample with at least min_words words for each example
        # this is an inefficient way to do this (since we regenerate for all inputs if just one is too short), but it works
        tries = 0
        while (m := min(len(x.split()) for x in decoded)) &lt; min_words and tries &lt;  self.config.neighborhood_config.top_p:
            if tries != 0:
                print()
                print(f&#34;min words: {m}, needed {min_words}, regenerating (try {tries})&#34;)

            sampling_kwargs = {}
            if self.config.do_top_p:
                sampling_kwargs[&#39;top_p&#39;] = self.config.top_p
            elif self.config.do_top_k:
                sampling_kwargs[&#39;top_k&#39;] = self.config.top_k
            #min_length = 50 if config.dataset_member in [&#39;pubmed&#39;] else 150

            #outputs = base_model.generate(**all_encoded, min_length=min_length, max_length=max_length, do_sample=True, **sampling_kwargs, pad_token_id=base_tokenizer.eos_token_id, eos_token_id=base_tokenizer.eos_token_id)
            #removed minlen and attention mask min_length=min_length, max_length=200, do_sample=True,pad_token_id=base_tokenizer.eos_token_id,
            outputs = self.model.generate(**all_encoded, min_length=min_words*2, max_length=max_words*3,  **sampling_kwargs,  eos_token_id=self.tokenizer.eos_token_id)
            decoded = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)
            tries += 1

        return decoded

    @torch.no_grad()
    def get_entropy(self, text: str):
        &#34;&#34;&#34;
            Get average entropy of each token in the text
        &#34;&#34;&#34;
        # raise NotImplementedError(&#34;get_entropy not implemented for OpenAI models&#34;)
        
        tokenized = self.tokenizer(text, return_tensors=&#34;pt&#34;).to(self.device)
        logits = self.model(**tokenized).logits[:,:-1]
        neg_entropy = F.softmax(logits, dim=-1) * F.log_softmax(logits, dim=-1)
        return -neg_entropy.sum(-1).mean().item()
    
    @torch.no_grad()
    def get_max_norm(self, text: str, context_len=None, tk_freq_map=None):
        # TODO: update like other attacks
        tokenized = self.tokenizer(
            text, return_tensors=&#34;pt&#34;).to(self.device)
        labels = tokenized.input_ids

        max_length = context_len if context_len is not None else self.max_length
        stride = max_length // 2 #self.stride
        all_prob = []
        for i in range(0, labels.size(1), stride):
            begin_loc = max(i + stride - max_length, 0)
            end_loc = min(i + stride, labels.size(1))
            trg_len = end_loc - i  # may be different from stride on last loop
            input_ids = labels[:, begin_loc:end_loc]
            target_ids = input_ids.clone()
            target_ids[:, :-trg_len] = -100

            outputs = self.model(input_ids, labels=target_ids)
            logits = outputs.logits
            # Shift so that tokens &lt; n predict n
            # print(logits.shape)
            shift_logits = logits[..., :-1, :].contiguous()
            # shift_logits = torch.transpose(shift_logits, 1, 2)
            probabilities = torch.nn.functional.log_softmax(shift_logits, dim=-1)
            shift_labels = target_ids[..., 1:].contiguous()
            labels_processed = shift_labels[0]

            for i, token_id in enumerate(labels_processed):
                if token_id != -100:
                    probability = probabilities[0, i, token_id].item()
                    max_tk_prob = torch.max(probabilities[0, i]).item()
                    tk_weight = max(tk_freq_map[token_id.item()], 1) / sum(tk_freq_map.values()) if tk_freq_map is not None else 1
                    if tk_weight == 0:
                        print(&#34;0 count token&#34;, token_id.item())
                    tk_norm = tk_weight
                    all_prob.append((1 - (max_tk_prob - probability)) / tk_norm)

        # Should be equal to # of tokens - 1 to account for shift
        assert len(all_prob) == labels.size(1) - 1
        return -np.mean(all_prob)


class OpenAI_APIModel(LanguageModel):
    &#34;&#34;&#34;
        Wrapper for OpenAI API calls
    &#34;&#34;&#34;
    def __init__(self, config: ExperimentConfig, **kwargs):
        super().__init__(config, **kwargs)
        self.model = None
        self.tokenizer = transformers.GPT2Tokenizer.from_pretrained(&#39;gpt2&#39;, cache_dir=self.cache_dir)
        self.API_TOKEN_COUNTER = 0
    
    @property
    def api_calls(self):
        &#34;&#34;&#34;
            Get the number of tokens used in API calls
        &#34;&#34;&#34;
        return self.API_TOKEN_COUNTER

    @torch.no_grad()
    def get_ll(self, text: str):
        &#34;&#34;&#34;
            Get the log likelihood of each text under the base_model
        &#34;&#34;&#34;
        openai_config = self.config.openai_config

        kwargs = {&#34;engine&#34;: openai_config.model, &#34;temperature&#34;: 0, &#34;max_tokens&#34;: 0, &#34;echo&#34;: True, &#34;logprobs&#34;: 0}
        r = openai.Completion.create(prompt=f&#34;&lt;|endoftext|&gt;{text}&#34;, **kwargs)
        result = r[&#39;choices&#39;][0]
        tokens, logprobs = result[&#34;logprobs&#34;][&#34;tokens&#34;][1:], result[&#34;logprobs&#34;][&#34;token_logprobs&#34;][1:]

        assert len(tokens) == len(logprobs), f&#34;Expected {len(tokens)} logprobs, got {len(logprobs)}&#34;

        return np.mean(logprobs)

    @torch.no_grad()
    def get_ref(self, text: str, ref_model: ReferenceModel):
        &#34;&#34;&#34;
            Get the  likelihood ratio of each text under the base_model -- MIA baseline
        &#34;&#34;&#34;
        raise NotImplementedError(&#34;OpenAI model not implemented for LIRA&#34;)
        openai_config = self.config.openai_config
        kwargs = {&#34;engine&#34;: openai_config.model, &#34;temperature&#34;: 0,
                    &#34;max_tokens&#34;: 0, &#34;echo&#34;: True, &#34;logprobs&#34;: 0}
        r = openai.Completion.create(prompt=f&#34;&lt;|endoftext|&gt;{text}&#34;, **kwargs)
        result = r[&#39;choices&#39;][0]
        tokens, logprobs = result[&#34;logprobs&#34;][&#34;tokens&#34;][1:], result[&#34;logprobs&#34;][&#34;token_logprobs&#34;][1:]

        assert len(tokens) == len(logprobs), f&#34;Expected {len(tokens)} logprobs, got {len(logprobs)}&#34;

        return np.mean(logprobs)

    def get_lls(self, texts: str):

        # use GPT2_TOKENIZER to get total number of tokens
        total_tokens = sum(len(self.tokenizer.encode(text)) for text in texts)
        self.API_TOKEN_COUNTER += total_tokens * 2  # multiply by two because OpenAI double-counts echo_prompt tokens

        pool = ThreadPool(self.config.batch_size)
        return pool.map(self.get_ll, texts)

    def _openai_sample(self, p: str):
        openai_config = self.config.openai_config
        if self.config.dataset_member != &#39;pubmed&#39;:  # keep Answer: prefix for pubmed
            p = drop_last_word(p)

        # sample from the openai model
        kwargs = { &#34;engine&#34;: openai_config.model, &#34;max_tokens&#34;: 200 }
        if self.config.do_top_p:
            kwargs[&#39;top_p&#39;] = self.config.top_p
    
        r = openai.Completion.create(prompt=f&#34;{p}&#34;, **kwargs)
        return p + r[&#39;choices&#39;][0].text


    def sample_from_model(self, texts: List[str], **kwargs):
        &#34;&#34;&#34;
            Sample from base_model using ****only**** the first 30 tokens in each example as context
        &#34;&#34;&#34;
        prompt_tokens = kwargs.get(&#39;prompt_tokens&#39;, 30)
        base_tokenizer = kwargs.get(&#39;base_tokenizer&#39;, None)
        if base_tokenizer is None:
            raise ValueError(&#34;Please provide base_tokenizer&#34;)

        # encode each text as a list of token ids
        if self.config.dataset_member == &#39;pubmed&#39;:
            texts = [t[:t.index(SEPARATOR)] for t in texts]
            all_encoded = base_tokenizer(texts, return_tensors=&#34;pt&#34;, padding=True).to(self.device)
        else:
            all_encoded = base_tokenizer(texts, return_tensors=&#34;pt&#34;, padding=True).to(self.device)
            all_encoded = {key: value[:, :prompt_tokens] for key, value in all_encoded.items()}

        # decode the prefixes back into text
        prefixes = base_tokenizer.batch_decode(all_encoded[&#39;input_ids&#39;], skip_special_tokens=True)
        pool = ThreadPool(self.config.batch_size)

        decoded = pool.map(self._openai_sample, prefixes)

        # count total number of tokens with GPT2_TOKENIZER
        total_tokens = sum(len(self.tokenizer.encode(x)) for x in decoded)
        self.API_TOKEN_COUNTER += total_tokens

        return decoded
    
    @torch.no_grad()
    def get_entropy(self, text: str):
        &#34;&#34;&#34;
            Get average entropy of each token in the text
        &#34;&#34;&#34;
        raise NotImplementedError(&#34;get_entropy not implemented for OpenAI models&#34;)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="mimir.models.LanguageModel"><code class="flex name class">
<span>class <span class="ident">LanguageModel</span></span>
<span>(</span><span>config: <a title="mimir.config.ExperimentConfig" href="config.html#mimir.config.ExperimentConfig">ExperimentConfig</a>, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Generic LM- used most often for target model</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LanguageModel(Model):
    &#34;&#34;&#34;
        Generic LM- used most often for target model
    &#34;&#34;&#34;
    def __init__(self, config: ExperimentConfig, **kwargs):
        super().__init__(config, **kwargs)
        self.device = self.config.env_config.device
        self.device_map = self.config.env_config.device_map
        # Use provided name (if provided)
        # Relevant for scoring-model scenario
        self.name = self.kwargs.get(&#39;name&#39;, self.config.base_model)

        base_model_kwargs = {}
        if config.revision:
            base_model_kwargs.update(dict(revision=config.revision))
        if &#39;gpt-j&#39; in self.name or &#39;neox&#39; in self.name:
            base_model_kwargs.update(dict(torch_dtype=torch.float16))
        if &#39;gpt-j&#39; in self.name:
            base_model_kwargs.update(dict(revision=&#39;float16&#39;))
        self.model, self.tokenizer = self.load_base_model_and_tokenizer(
            model_kwargs=base_model_kwargs)
        self.load_model_properties()

    @torch.no_grad()
    def get_ref(self, text: str, ref_model: ReferenceModel, tokens=None, probs=None):
        &#34;&#34;&#34;
            Compute the loss of a given text calibrated against the text&#39;s loss under a reference model -- MIA baseline
        &#34;&#34;&#34;
        lls = self.get_ll(text, tokens=tokens, probs=probs)
        lls_ref = ref_model.get_ll(text)

        return lls - lls_ref

    @torch.no_grad()
    def get_rank(self, text: str, log: bool=False):
        &#34;&#34;&#34;
            Get the average rank of each observed token sorted by model likelihood
        &#34;&#34;&#34;
        openai_config = self.config.openai_config
        assert openai_config is None, &#34;get_rank not implemented for OpenAI models&#34;

        tokenized = self.tokenizer(text, return_tensors=&#34;pt&#34;).to(self.device)
        logits = self.model(**tokenized).logits[:,:-1]
        labels = tokenized.input_ids[:,1:]

        # get rank of each label token in the model&#39;s likelihood ordering
        matches = (logits.argsort(-1, descending=True) == labels.unsqueeze(-1)).nonzero()

        assert matches.shape[1] == 3, f&#34;Expected 3 dimensions in matches tensor, got {matches.shape}&#34;

        ranks, timesteps = matches[:,-1], matches[:,-2]

        # make sure we got exactly one match for each timestep in the sequence
        assert (timesteps == torch.arange(len(timesteps)).to(timesteps.device)).all(), &#34;Expected one match per timestep&#34;

        ranks = ranks.float() + 1 # convert to 1-indexed rank
        if log:
            ranks = torch.log(ranks)

        return ranks.float().mean().item()

    # TODO extend for longer sequences
    @torch.no_grad()
    def get_lls(self, texts: List[str], batch_size: int = 6):
        #return [self.get_ll(text) for text in texts] # -np.mean([self.get_ll(text) for text in texts])
        # tokenized = self.tokenizer(texts, return_tensors=&#34;pt&#34;, padding=True)
        # labels = tokenized.input_ids
        total_size = len(texts)
        losses = []
        for i in range(0, total_size, batch_size):
            # Delegate batches and tokenize
            batch = texts[i:i+batch_size]
            tokenized = self.tokenizer(batch, return_tensors=&#34;pt&#34;, padding=True, return_attention_mask=True)
            label_batch = tokenized.input_ids
            
            # # mask out padding tokens
            attention_mask = tokenized.attention_mask
            assert attention_mask.size() == label_batch.size()

            needs_sliding = label_batch.size(1) &gt; self.max_length // 2
            if not needs_sliding:
                label_batch = label_batch.to(self.device)
                attention_mask = attention_mask.to(self.device)

            # Collect token probabilities per sample in batch
            all_prob = defaultdict(list)
            for i in range(0, label_batch.size(1), self.stride):
                begin_loc = max(i + self.stride - self.max_length, 0)
                end_loc = min(i + self.stride, label_batch.size(1))
                trg_len = end_loc - i  # may be different from stride on last loop
                input_ids = label_batch[:, begin_loc:end_loc]
                mask = attention_mask[:, begin_loc:end_loc]
                if needs_sliding:
                    input_ids = input_ids.to(self.device)
                    mask = mask.to(self.device)
                    
                target_ids = input_ids.clone()
                # Don&#39;t count padded tokens or tokens that already have computed probabilities
                target_ids[:, :-trg_len] = -100
                # target_ids[attention_mask == 0] = -100
                
                logits = self.model(input_ids, labels=target_ids, attention_mask=mask).logits.cpu()
                target_ids = target_ids.cpu()
                shift_logits = logits[..., :-1, :].contiguous()
                probabilities = torch.nn.functional.log_softmax(shift_logits, dim=-1)
                shift_labels = target_ids[..., 1:].contiguous()

                for i, sample in enumerate(shift_labels):
                    for j, token_id in enumerate(sample):
                        if token_id != -100 and token_id != self.tokenizer.pad_token_id:
                            probability = probabilities[i, j, token_id].item()
                            all_prob[i].append(probability)

                del input_ids
                del mask
            
            # average over each sample to get losses
            batch_losses = [-np.mean(all_prob[idx]) for idx in range(label_batch.size(0))]
            # print(batch_losses)
            losses.extend(batch_losses)
            del label_batch
            del attention_mask
        return losses #np.mean(losses)

    def sample_from_model(self, texts: List[str], **kwargs):
        &#34;&#34;&#34;
            Sample from base_model using ****only**** the first 30 tokens in each example as context
        &#34;&#34;&#34;
        min_words = kwargs.get(&#39;min_words&#39;, 55)
        max_words = kwargs.get(&#39;max_words&#39;, 200)
        prompt_tokens = kwargs.get(&#39;prompt_tokens&#39;, 30)

        # encode each text as a list of token ids
        if self.config.dataset_member == &#39;pubmed&#39;:
            texts = [t[:t.index(SEPARATOR)] for t in texts]
            all_encoded = self.tokenizer(texts, return_tensors=&#34;pt&#34;, padding=True).to(self.device, non_blocking=True)
        else:
            all_encoded = self.tokenizer(texts, return_tensors=&#34;pt&#34;, padding=True).to(self.device, non_blocking=True)
            all_encoded = {key: value[:, :prompt_tokens] for key, value in all_encoded.items()}

        decoded = [&#39;&#39; for _ in range(len(texts))]

        # sample from the model until we get a sample with at least min_words words for each example
        # this is an inefficient way to do this (since we regenerate for all inputs if just one is too short), but it works
        tries = 0
        while (m := min(len(x.split()) for x in decoded)) &lt; min_words and tries &lt;  self.config.neighborhood_config.top_p:
            if tries != 0:
                print()
                print(f&#34;min words: {m}, needed {min_words}, regenerating (try {tries})&#34;)

            sampling_kwargs = {}
            if self.config.do_top_p:
                sampling_kwargs[&#39;top_p&#39;] = self.config.top_p
            elif self.config.do_top_k:
                sampling_kwargs[&#39;top_k&#39;] = self.config.top_k
            #min_length = 50 if config.dataset_member in [&#39;pubmed&#39;] else 150

            #outputs = base_model.generate(**all_encoded, min_length=min_length, max_length=max_length, do_sample=True, **sampling_kwargs, pad_token_id=base_tokenizer.eos_token_id, eos_token_id=base_tokenizer.eos_token_id)
            #removed minlen and attention mask min_length=min_length, max_length=200, do_sample=True,pad_token_id=base_tokenizer.eos_token_id,
            outputs = self.model.generate(**all_encoded, min_length=min_words*2, max_length=max_words*3,  **sampling_kwargs,  eos_token_id=self.tokenizer.eos_token_id)
            decoded = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)
            tries += 1

        return decoded

    @torch.no_grad()
    def get_entropy(self, text: str):
        &#34;&#34;&#34;
            Get average entropy of each token in the text
        &#34;&#34;&#34;
        # raise NotImplementedError(&#34;get_entropy not implemented for OpenAI models&#34;)
        
        tokenized = self.tokenizer(text, return_tensors=&#34;pt&#34;).to(self.device)
        logits = self.model(**tokenized).logits[:,:-1]
        neg_entropy = F.softmax(logits, dim=-1) * F.log_softmax(logits, dim=-1)
        return -neg_entropy.sum(-1).mean().item()
    
    @torch.no_grad()
    def get_max_norm(self, text: str, context_len=None, tk_freq_map=None):
        # TODO: update like other attacks
        tokenized = self.tokenizer(
            text, return_tensors=&#34;pt&#34;).to(self.device)
        labels = tokenized.input_ids

        max_length = context_len if context_len is not None else self.max_length
        stride = max_length // 2 #self.stride
        all_prob = []
        for i in range(0, labels.size(1), stride):
            begin_loc = max(i + stride - max_length, 0)
            end_loc = min(i + stride, labels.size(1))
            trg_len = end_loc - i  # may be different from stride on last loop
            input_ids = labels[:, begin_loc:end_loc]
            target_ids = input_ids.clone()
            target_ids[:, :-trg_len] = -100

            outputs = self.model(input_ids, labels=target_ids)
            logits = outputs.logits
            # Shift so that tokens &lt; n predict n
            # print(logits.shape)
            shift_logits = logits[..., :-1, :].contiguous()
            # shift_logits = torch.transpose(shift_logits, 1, 2)
            probabilities = torch.nn.functional.log_softmax(shift_logits, dim=-1)
            shift_labels = target_ids[..., 1:].contiguous()
            labels_processed = shift_labels[0]

            for i, token_id in enumerate(labels_processed):
                if token_id != -100:
                    probability = probabilities[0, i, token_id].item()
                    max_tk_prob = torch.max(probabilities[0, i]).item()
                    tk_weight = max(tk_freq_map[token_id.item()], 1) / sum(tk_freq_map.values()) if tk_freq_map is not None else 1
                    if tk_weight == 0:
                        print(&#34;0 count token&#34;, token_id.item())
                    tk_norm = tk_weight
                    all_prob.append((1 - (max_tk_prob - probability)) / tk_norm)

        # Should be equal to # of tokens - 1 to account for shift
        assert len(all_prob) == labels.size(1) - 1
        return -np.mean(all_prob)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mimir.models.Model" href="#mimir.models.Model">Model</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="mimir.models.OpenAI_APIModel" href="#mimir.models.OpenAI_APIModel">OpenAI_APIModel</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="mimir.models.LanguageModel.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="mimir.models.LanguageModel.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="mimir.models.LanguageModel.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="mimir.models.LanguageModel.get_entropy"><code class="name flex">
<span>def <span class="ident">get_entropy</span></span>(<span>self, text: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Get average entropy of each token in the text</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.no_grad()
def get_entropy(self, text: str):
    &#34;&#34;&#34;
        Get average entropy of each token in the text
    &#34;&#34;&#34;
    # raise NotImplementedError(&#34;get_entropy not implemented for OpenAI models&#34;)
    
    tokenized = self.tokenizer(text, return_tensors=&#34;pt&#34;).to(self.device)
    logits = self.model(**tokenized).logits[:,:-1]
    neg_entropy = F.softmax(logits, dim=-1) * F.log_softmax(logits, dim=-1)
    return -neg_entropy.sum(-1).mean().item()</code></pre>
</details>
</dd>
<dt id="mimir.models.LanguageModel.get_lls"><code class="name flex">
<span>def <span class="ident">get_lls</span></span>(<span>self, texts: List[str], batch_size: int = 6)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.no_grad()
def get_lls(self, texts: List[str], batch_size: int = 6):
    #return [self.get_ll(text) for text in texts] # -np.mean([self.get_ll(text) for text in texts])
    # tokenized = self.tokenizer(texts, return_tensors=&#34;pt&#34;, padding=True)
    # labels = tokenized.input_ids
    total_size = len(texts)
    losses = []
    for i in range(0, total_size, batch_size):
        # Delegate batches and tokenize
        batch = texts[i:i+batch_size]
        tokenized = self.tokenizer(batch, return_tensors=&#34;pt&#34;, padding=True, return_attention_mask=True)
        label_batch = tokenized.input_ids
        
        # # mask out padding tokens
        attention_mask = tokenized.attention_mask
        assert attention_mask.size() == label_batch.size()

        needs_sliding = label_batch.size(1) &gt; self.max_length // 2
        if not needs_sliding:
            label_batch = label_batch.to(self.device)
            attention_mask = attention_mask.to(self.device)

        # Collect token probabilities per sample in batch
        all_prob = defaultdict(list)
        for i in range(0, label_batch.size(1), self.stride):
            begin_loc = max(i + self.stride - self.max_length, 0)
            end_loc = min(i + self.stride, label_batch.size(1))
            trg_len = end_loc - i  # may be different from stride on last loop
            input_ids = label_batch[:, begin_loc:end_loc]
            mask = attention_mask[:, begin_loc:end_loc]
            if needs_sliding:
                input_ids = input_ids.to(self.device)
                mask = mask.to(self.device)
                
            target_ids = input_ids.clone()
            # Don&#39;t count padded tokens or tokens that already have computed probabilities
            target_ids[:, :-trg_len] = -100
            # target_ids[attention_mask == 0] = -100
            
            logits = self.model(input_ids, labels=target_ids, attention_mask=mask).logits.cpu()
            target_ids = target_ids.cpu()
            shift_logits = logits[..., :-1, :].contiguous()
            probabilities = torch.nn.functional.log_softmax(shift_logits, dim=-1)
            shift_labels = target_ids[..., 1:].contiguous()

            for i, sample in enumerate(shift_labels):
                for j, token_id in enumerate(sample):
                    if token_id != -100 and token_id != self.tokenizer.pad_token_id:
                        probability = probabilities[i, j, token_id].item()
                        all_prob[i].append(probability)

            del input_ids
            del mask
        
        # average over each sample to get losses
        batch_losses = [-np.mean(all_prob[idx]) for idx in range(label_batch.size(0))]
        # print(batch_losses)
        losses.extend(batch_losses)
        del label_batch
        del attention_mask
    return losses #np.mean(losses)</code></pre>
</details>
</dd>
<dt id="mimir.models.LanguageModel.get_max_norm"><code class="name flex">
<span>def <span class="ident">get_max_norm</span></span>(<span>self, text: str, context_len=None, tk_freq_map=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.no_grad()
def get_max_norm(self, text: str, context_len=None, tk_freq_map=None):
    # TODO: update like other attacks
    tokenized = self.tokenizer(
        text, return_tensors=&#34;pt&#34;).to(self.device)
    labels = tokenized.input_ids

    max_length = context_len if context_len is not None else self.max_length
    stride = max_length // 2 #self.stride
    all_prob = []
    for i in range(0, labels.size(1), stride):
        begin_loc = max(i + stride - max_length, 0)
        end_loc = min(i + stride, labels.size(1))
        trg_len = end_loc - i  # may be different from stride on last loop
        input_ids = labels[:, begin_loc:end_loc]
        target_ids = input_ids.clone()
        target_ids[:, :-trg_len] = -100

        outputs = self.model(input_ids, labels=target_ids)
        logits = outputs.logits
        # Shift so that tokens &lt; n predict n
        # print(logits.shape)
        shift_logits = logits[..., :-1, :].contiguous()
        # shift_logits = torch.transpose(shift_logits, 1, 2)
        probabilities = torch.nn.functional.log_softmax(shift_logits, dim=-1)
        shift_labels = target_ids[..., 1:].contiguous()
        labels_processed = shift_labels[0]

        for i, token_id in enumerate(labels_processed):
            if token_id != -100:
                probability = probabilities[0, i, token_id].item()
                max_tk_prob = torch.max(probabilities[0, i]).item()
                tk_weight = max(tk_freq_map[token_id.item()], 1) / sum(tk_freq_map.values()) if tk_freq_map is not None else 1
                if tk_weight == 0:
                    print(&#34;0 count token&#34;, token_id.item())
                tk_norm = tk_weight
                all_prob.append((1 - (max_tk_prob - probability)) / tk_norm)

    # Should be equal to # of tokens - 1 to account for shift
    assert len(all_prob) == labels.size(1) - 1
    return -np.mean(all_prob)</code></pre>
</details>
</dd>
<dt id="mimir.models.LanguageModel.get_rank"><code class="name flex">
<span>def <span class="ident">get_rank</span></span>(<span>self, text: str, log: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the average rank of each observed token sorted by model likelihood</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.no_grad()
def get_rank(self, text: str, log: bool=False):
    &#34;&#34;&#34;
        Get the average rank of each observed token sorted by model likelihood
    &#34;&#34;&#34;
    openai_config = self.config.openai_config
    assert openai_config is None, &#34;get_rank not implemented for OpenAI models&#34;

    tokenized = self.tokenizer(text, return_tensors=&#34;pt&#34;).to(self.device)
    logits = self.model(**tokenized).logits[:,:-1]
    labels = tokenized.input_ids[:,1:]

    # get rank of each label token in the model&#39;s likelihood ordering
    matches = (logits.argsort(-1, descending=True) == labels.unsqueeze(-1)).nonzero()

    assert matches.shape[1] == 3, f&#34;Expected 3 dimensions in matches tensor, got {matches.shape}&#34;

    ranks, timesteps = matches[:,-1], matches[:,-2]

    # make sure we got exactly one match for each timestep in the sequence
    assert (timesteps == torch.arange(len(timesteps)).to(timesteps.device)).all(), &#34;Expected one match per timestep&#34;

    ranks = ranks.float() + 1 # convert to 1-indexed rank
    if log:
        ranks = torch.log(ranks)

    return ranks.float().mean().item()</code></pre>
</details>
</dd>
<dt id="mimir.models.LanguageModel.get_ref"><code class="name flex">
<span>def <span class="ident">get_ref</span></span>(<span>self, text: str, ref_model: <a title="mimir.models.ReferenceModel" href="#mimir.models.ReferenceModel">ReferenceModel</a>, tokens=None, probs=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the loss of a given text calibrated against the text's loss under a reference model &ndash; MIA baseline</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.no_grad()
def get_ref(self, text: str, ref_model: ReferenceModel, tokens=None, probs=None):
    &#34;&#34;&#34;
        Compute the loss of a given text calibrated against the text&#39;s loss under a reference model -- MIA baseline
    &#34;&#34;&#34;
    lls = self.get_ll(text, tokens=tokens, probs=probs)
    lls_ref = ref_model.get_ll(text)

    return lls - lls_ref</code></pre>
</details>
</dd>
<dt id="mimir.models.LanguageModel.sample_from_model"><code class="name flex">
<span>def <span class="ident">sample_from_model</span></span>(<span>self, texts: List[str], **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Sample from base_model using <strong><em>*only</em></strong>* the first 30 tokens in each example as context</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_from_model(self, texts: List[str], **kwargs):
    &#34;&#34;&#34;
        Sample from base_model using ****only**** the first 30 tokens in each example as context
    &#34;&#34;&#34;
    min_words = kwargs.get(&#39;min_words&#39;, 55)
    max_words = kwargs.get(&#39;max_words&#39;, 200)
    prompt_tokens = kwargs.get(&#39;prompt_tokens&#39;, 30)

    # encode each text as a list of token ids
    if self.config.dataset_member == &#39;pubmed&#39;:
        texts = [t[:t.index(SEPARATOR)] for t in texts]
        all_encoded = self.tokenizer(texts, return_tensors=&#34;pt&#34;, padding=True).to(self.device, non_blocking=True)
    else:
        all_encoded = self.tokenizer(texts, return_tensors=&#34;pt&#34;, padding=True).to(self.device, non_blocking=True)
        all_encoded = {key: value[:, :prompt_tokens] for key, value in all_encoded.items()}

    decoded = [&#39;&#39; for _ in range(len(texts))]

    # sample from the model until we get a sample with at least min_words words for each example
    # this is an inefficient way to do this (since we regenerate for all inputs if just one is too short), but it works
    tries = 0
    while (m := min(len(x.split()) for x in decoded)) &lt; min_words and tries &lt;  self.config.neighborhood_config.top_p:
        if tries != 0:
            print()
            print(f&#34;min words: {m}, needed {min_words}, regenerating (try {tries})&#34;)

        sampling_kwargs = {}
        if self.config.do_top_p:
            sampling_kwargs[&#39;top_p&#39;] = self.config.top_p
        elif self.config.do_top_k:
            sampling_kwargs[&#39;top_k&#39;] = self.config.top_k
        #min_length = 50 if config.dataset_member in [&#39;pubmed&#39;] else 150

        #outputs = base_model.generate(**all_encoded, min_length=min_length, max_length=max_length, do_sample=True, **sampling_kwargs, pad_token_id=base_tokenizer.eos_token_id, eos_token_id=base_tokenizer.eos_token_id)
        #removed minlen and attention mask min_length=min_length, max_length=200, do_sample=True,pad_token_id=base_tokenizer.eos_token_id,
        outputs = self.model.generate(**all_encoded, min_length=min_words*2, max_length=max_words*3,  **sampling_kwargs,  eos_token_id=self.tokenizer.eos_token_id)
        decoded = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)
        tries += 1

    return decoded</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mimir.models.Model" href="#mimir.models.Model">Model</a></b></code>:
<ul class="hlist">
<li><code><a title="mimir.models.Model.forward" href="#mimir.models.Model.forward">forward</a></code></li>
<li><code><a title="mimir.models.Model.get_ll" href="#mimir.models.Model.get_ll">get_ll</a></code></li>
<li><code><a title="mimir.models.Model.get_probabilities" href="#mimir.models.Model.get_probabilities">get_probabilities</a></code></li>
<li><code><a title="mimir.models.Model.load" href="#mimir.models.Model.load">load</a></code></li>
<li><code><a title="mimir.models.Model.load_base_model_and_tokenizer" href="#mimir.models.Model.load_base_model_and_tokenizer">load_base_model_and_tokenizer</a></code></li>
<li><code><a title="mimir.models.Model.load_model_properties" href="#mimir.models.Model.load_model_properties">load_model_properties</a></code></li>
<li><code><a title="mimir.models.Model.to" href="#mimir.models.Model.to">to</a></code></li>
<li><code><a title="mimir.models.Model.unload" href="#mimir.models.Model.unload">unload</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mimir.models.Model"><code class="flex name class">
<span>class <span class="ident">Model</span></span>
<span>(</span><span>config: <a title="mimir.config.ExperimentConfig" href="config.html#mimir.config.ExperimentConfig">ExperimentConfig</a>, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class (for LLMs).</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Model(nn.Module):
    &#34;&#34;&#34;
        Base class (for LLMs).
    &#34;&#34;&#34;
    def __init__(self, config: ExperimentConfig, **kwargs):
        super().__init__()
        self.model = None # Set by child class
        self.tokenizer = None # Set by child class
        self.config = config
        self.device = None
        self.device_map = None
        self.name = None
        self.kwargs = kwargs
        self.cache_dir = self.config.env_config.cache_dir

    def to(self, device):
        &#34;&#34;&#34;
            Shift model to a particular device.
        &#34;&#34;&#34;
        self.model.to(device, non_blocking=True)

    def load(self):
        &#34;&#34;&#34;
            Load model onto GPU (and compile, if requested) if not already loaded with device map.
        &#34;&#34;&#34;
        if not self.device_map:
            start = time.time()
            try:
                self.model.cpu()
            except NameError:
                pass
            if self.config.openai_config is None:
                self.model.to(self.device, non_blocking=True)
            if self.config.env_config.compile:
                torch.compile(self.model)
            print(f&#39;DONE ({time.time() - start:.2f}s)&#39;)

    def unload(self):
        &#34;&#34;&#34;
            Unload model from GPU
        &#34;&#34;&#34;
        start = time.time()
        try:
            self.model.cpu()
        except NameError:
            pass
        print(f&#39;DONE ({time.time() - start:.2f}s)&#39;)

    def get_probabilities(self,
                          text: str,
                          tokens: np.ndarray = None,
                          no_grads: bool = True):
        &#34;&#34;&#34;
            Get the probabilities or log-softmaxed logits for a text under the current model.
            Args:
                text (str): The input text for which to calculate probabilities.
                tokens (numpy.ndarray, optional): An optional array of token ids. If provided, these tokens
                are used instead of tokenizing the input text. Defaults to None.

            Raises:
                ValueError: If the device or name attributes of the instance are not set.

            Returns:
                list: A list of probabilities.
        &#34;&#34;&#34;
        with torch.set_grad_enabled(not no_grads):
            if self.device is None or self.name is None:
                raise ValueError(&#34;Please set self.device and self.name in child class&#34;)

            if tokens is not None:
                labels = torch.from_numpy(tokens.astype(np.int64)).type(torch.LongTensor)
                if labels.shape[0] != 1:
                    # expand first dimension
                    labels = labels.unsqueeze(0)
            else:
                tokenized = self.tokenizer(
                    text, return_tensors=&#34;pt&#34;)
                labels = tokenized.input_ids

            all_prob = []
            for i in range(0, labels.size(1), self.stride):
                begin_loc = max(i + self.stride - self.max_length, 0)
                end_loc = min(i + self.stride, labels.size(1))
                trg_len = end_loc - i  # may be different from stride on last loop
                input_ids = labels[:, begin_loc:end_loc].to(self.device)
                target_ids = input_ids.clone()
                target_ids[:, :-trg_len] = -100

                logits = self.model(input_ids, labels=target_ids).logits
                if no_grads:
                    logits = logits.cpu()
                shift_logits = logits[..., :-1, :].contiguous()
                probabilities = torch.nn.functional.log_softmax(shift_logits, dim=-1)
                shift_labels = target_ids[..., 1:]
                if no_grads:
                    shift_labels = shift_labels.cpu()
                shift_labels = shift_labels.contiguous()
                labels_processed = shift_labels[0]

                del input_ids
                del target_ids

                for i, token_id in enumerate(labels_processed):
                    if token_id != -100:
                        probability = probabilities[0, i, token_id]
                        if no_grads:
                            probability = probability.item()
                        all_prob.append(probability)
            # Should be equal to # of tokens - 1 to account for shift
            assert len(all_prob) == labels.size(1) - 1

        if not no_grads:
            all_prob = torch.stack(all_prob)

        return all_prob

    @torch.no_grad()
    def get_ll(self,
               text: str,
               tokens: np.ndarray=None,
               probs = None):
        &#34;&#34;&#34;
            Get the log likelihood of each text under the base_model.

            Args:
                text (str): The input text for which to calculate the log likelihood.
                tokens (numpy.ndarray, optional): An optional array of token ids. If provided, these tokens
                are used instead of tokenizing the input text. Defaults to None.
                probs (list, optional): An optional list of probabilities. If provided, these probabilities
                are used instead of calling the `get_probabilities` method. Defaults to None.
        &#34;&#34;&#34;
        all_prob = probs if probs is not None else self.get_probabilities(text, tokens=tokens)
        return -np.mean(all_prob)

    def load_base_model_and_tokenizer(self, model_kwargs):
        &#34;&#34;&#34;
            Load the base model and tokenizer for a given model name.
        &#34;&#34;&#34;
        if self.device is None or self.name is None:
            raise ValueError(&#34;Please set self.device and self.name in child class&#34;)

        if self.config.openai_config is None:
            print(f&#39;Loading BASE model {self.name}...&#39;)
            device_map = self.device_map # if self.device_map else &#39;cpu&#39;
            if &#34;silo&#34; in self.name or &#34;balanced&#34; in self.name:
                from utils.transformers.model import OpenLMforCausalLM
                model = OpenLMforCausalLM.from_pretrained(
                    self.name, **model_kwargs, device_map=self.device, cache_dir=self.cache_dir)
                # Extract the model from the model wrapper so we dont need to call model.model
            elif &#34;llama&#34; in self.name or &#34;alpaca&#34; in self.name:
                # TODO: This should be smth specified in config in case user has
                # llama is too big, gotta use device map
                model = transformers.AutoModelForCausalLM.from_pretrained(self.name, **model_kwargs, device_map=&#34;balanced_low_0&#34;, cache_dir=self.cache_dir)
                self.device = &#39;cuda:1&#39;
            elif &#34;stablelm&#34; in self.name.lower():  # models requiring custom code
                model = transformers.AutoModelForCausalLM.from_pretrained(
                    self.name, **model_kwargs, trust_remote_code=True, device_map=device_map, cache_dir=self.cache_dir)
            elif &#34;olmo&#34; in self.name.lower():
                model = transformers.AutoModelForCausalLM.from_pretrained(
                    self.name, **model_kwargs, trust_remote_code=True, cache_dir=self.cache_dir)
            else:
                model = transformers.AutoModelForCausalLM.from_pretrained(
                    self.name, **model_kwargs, device_map=device_map, cache_dir=self.cache_dir)
        else:
            model = None

        optional_tok_kwargs = {}
        if &#34;facebook/opt-&#34; in self.name:
            print(&#34;Using non-fast tokenizer for OPT&#34;)
            optional_tok_kwargs[&#39;fast&#39;] = False
        if self.config.dataset_member in [&#39;pubmed&#39;] or self.config.dataset_nonmember in [&#39;pubmed&#39;]:
            optional_tok_kwargs[&#39;padding_side&#39;] = &#39;left&#39;
            self.pad_token = self.tokenizer.eos_token_id
        if &#34;silo&#34; in self.name or &#34;balanced&#34; in self.name:
            tokenizer = transformers.GPTNeoXTokenizerFast.from_pretrained(
                &#34;EleutherAI/gpt-neox-20b&#34;, **optional_tok_kwargs, cache_dir=self.cache_dir)
        elif &#34;datablations&#34; in self.name:
            tokenizer = transformers.AutoTokenizer.from_pretrained(
                &#34;gpt2&#34;, **optional_tok_kwargs, cache_dir=self.cache_dir)
        elif &#34;llama&#34; in self.name or &#34;alpaca&#34; in self.name:
            tokenizer = transformers.LlamaTokenizer.from_pretrained(
                self.name, **optional_tok_kwargs, cache_dir=self.cache_dir)
        elif &#34;pubmedgpt&#34; in self.name:
            tokenizer = transformers.AutoTokenizer.from_pretrained(
                &#34;stanford-crfm/BioMedLM&#34;, **optional_tok_kwargs, cache_dir=self.cache_dir)
        else:
            tokenizer = transformers.AutoTokenizer.from_pretrained(
                self.name, **optional_tok_kwargs, cache_dir=self.cache_dir,
                trust_remote_code=True if &#34;olmo&#34; in self.name.lower() else False)
        tokenizer.add_special_tokens({&#39;pad_token&#39;: &#39;[PAD]&#39;})

        return model, tokenizer

    def load_model_properties(self):
        &#34;&#34;&#34;
            Load model properties, such as max length and stride.
        &#34;&#34;&#34;
        # TODO: getting max_length of input could be more generic
        if &#34;silo&#34; in self.name or &#34;balanced&#34; in self.name:
            self.max_length = self.model.model.seq_len
        elif hasattr(self.model.config, &#39;max_position_embeddings&#39;):
            self.max_length = self.model.config.max_position_embeddings
        elif hasattr(self.model.config, &#39;n_positions&#39;):
            self.max_length = self.model.config.n_positions
        else:
            # Default window size
            self.max_length = 1024
        self.stride = self.max_length // 2</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="mimir.attacks.neighborhood.MaskFillingModel" href="attacks/neighborhood.html#mimir.attacks.neighborhood.MaskFillingModel">MaskFillingModel</a></li>
<li><a title="mimir.models.LanguageModel" href="#mimir.models.LanguageModel">LanguageModel</a></li>
<li><a title="mimir.models.QuantileReferenceModel" href="#mimir.models.QuantileReferenceModel">QuantileReferenceModel</a></li>
<li><a title="mimir.models.ReferenceModel" href="#mimir.models.ReferenceModel">ReferenceModel</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="mimir.models.Model.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="mimir.models.Model.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="mimir.models.Model.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="mimir.models.Model.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, *input: Any) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def _forward_unimplemented(self, *input: Any) -&gt; None:
    r&#34;&#34;&#34;Define the computation performed at every call.

    Should be overridden by all subclasses.

    .. note::
        Although the recipe for forward pass needs to be defined within
        this function, one should call the :class:`Module` instance afterwards
        instead of this since the former takes care of running the
        registered hooks while the latter silently ignores them.
    &#34;&#34;&#34;
    raise NotImplementedError(f&#34;Module [{type(self).__name__}] is missing the required \&#34;forward\&#34; function&#34;)</code></pre>
</details>
</dd>
<dt id="mimir.models.Model.get_ll"><code class="name flex">
<span>def <span class="ident">get_ll</span></span>(<span>self, text: str, tokens: numpy.ndarray = None, probs=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the log likelihood of each text under the base_model.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>text</code></strong> :&ensp;<code>str</code></dt>
<dd>The input text for which to calculate the log likelihood.</dd>
<dt><strong><code>tokens</code></strong> :&ensp;<code>numpy.ndarray</code>, optional</dt>
<dd>An optional array of token ids. If provided, these tokens</dd>
<dt>are used instead of tokenizing the input text. Defaults to None.</dt>
<dt><strong><code>probs</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>An optional list of probabilities. If provided, these probabilities</dd>
</dl>
<p>are used instead of calling the <code>get_probabilities</code> method. Defaults to None.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.no_grad()
def get_ll(self,
           text: str,
           tokens: np.ndarray=None,
           probs = None):
    &#34;&#34;&#34;
        Get the log likelihood of each text under the base_model.

        Args:
            text (str): The input text for which to calculate the log likelihood.
            tokens (numpy.ndarray, optional): An optional array of token ids. If provided, these tokens
            are used instead of tokenizing the input text. Defaults to None.
            probs (list, optional): An optional list of probabilities. If provided, these probabilities
            are used instead of calling the `get_probabilities` method. Defaults to None.
    &#34;&#34;&#34;
    all_prob = probs if probs is not None else self.get_probabilities(text, tokens=tokens)
    return -np.mean(all_prob)</code></pre>
</details>
</dd>
<dt id="mimir.models.Model.get_probabilities"><code class="name flex">
<span>def <span class="ident">get_probabilities</span></span>(<span>self, text: str, tokens: numpy.ndarray = None, no_grads: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the probabilities or log-softmaxed logits for a text under the current model.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>text</code></strong> :&ensp;<code>str</code></dt>
<dd>The input text for which to calculate probabilities.</dd>
<dt><strong><code>tokens</code></strong> :&ensp;<code>numpy.ndarray</code>, optional</dt>
<dd>An optional array of token ids. If provided, these tokens</dd>
</dl>
<p>are used instead of tokenizing the input text. Defaults to None.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If the device or name attributes of the instance are not set.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code></dt>
<dd>A list of probabilities.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_probabilities(self,
                      text: str,
                      tokens: np.ndarray = None,
                      no_grads: bool = True):
    &#34;&#34;&#34;
        Get the probabilities or log-softmaxed logits for a text under the current model.
        Args:
            text (str): The input text for which to calculate probabilities.
            tokens (numpy.ndarray, optional): An optional array of token ids. If provided, these tokens
            are used instead of tokenizing the input text. Defaults to None.

        Raises:
            ValueError: If the device or name attributes of the instance are not set.

        Returns:
            list: A list of probabilities.
    &#34;&#34;&#34;
    with torch.set_grad_enabled(not no_grads):
        if self.device is None or self.name is None:
            raise ValueError(&#34;Please set self.device and self.name in child class&#34;)

        if tokens is not None:
            labels = torch.from_numpy(tokens.astype(np.int64)).type(torch.LongTensor)
            if labels.shape[0] != 1:
                # expand first dimension
                labels = labels.unsqueeze(0)
        else:
            tokenized = self.tokenizer(
                text, return_tensors=&#34;pt&#34;)
            labels = tokenized.input_ids

        all_prob = []
        for i in range(0, labels.size(1), self.stride):
            begin_loc = max(i + self.stride - self.max_length, 0)
            end_loc = min(i + self.stride, labels.size(1))
            trg_len = end_loc - i  # may be different from stride on last loop
            input_ids = labels[:, begin_loc:end_loc].to(self.device)
            target_ids = input_ids.clone()
            target_ids[:, :-trg_len] = -100

            logits = self.model(input_ids, labels=target_ids).logits
            if no_grads:
                logits = logits.cpu()
            shift_logits = logits[..., :-1, :].contiguous()
            probabilities = torch.nn.functional.log_softmax(shift_logits, dim=-1)
            shift_labels = target_ids[..., 1:]
            if no_grads:
                shift_labels = shift_labels.cpu()
            shift_labels = shift_labels.contiguous()
            labels_processed = shift_labels[0]

            del input_ids
            del target_ids

            for i, token_id in enumerate(labels_processed):
                if token_id != -100:
                    probability = probabilities[0, i, token_id]
                    if no_grads:
                        probability = probability.item()
                    all_prob.append(probability)
        # Should be equal to # of tokens - 1 to account for shift
        assert len(all_prob) == labels.size(1) - 1

    if not no_grads:
        all_prob = torch.stack(all_prob)

    return all_prob</code></pre>
</details>
</dd>
<dt id="mimir.models.Model.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Load model onto GPU (and compile, if requested) if not already loaded with device map.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load(self):
    &#34;&#34;&#34;
        Load model onto GPU (and compile, if requested) if not already loaded with device map.
    &#34;&#34;&#34;
    if not self.device_map:
        start = time.time()
        try:
            self.model.cpu()
        except NameError:
            pass
        if self.config.openai_config is None:
            self.model.to(self.device, non_blocking=True)
        if self.config.env_config.compile:
            torch.compile(self.model)
        print(f&#39;DONE ({time.time() - start:.2f}s)&#39;)</code></pre>
</details>
</dd>
<dt id="mimir.models.Model.load_base_model_and_tokenizer"><code class="name flex">
<span>def <span class="ident">load_base_model_and_tokenizer</span></span>(<span>self, model_kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Load the base model and tokenizer for a given model name.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_base_model_and_tokenizer(self, model_kwargs):
    &#34;&#34;&#34;
        Load the base model and tokenizer for a given model name.
    &#34;&#34;&#34;
    if self.device is None or self.name is None:
        raise ValueError(&#34;Please set self.device and self.name in child class&#34;)

    if self.config.openai_config is None:
        print(f&#39;Loading BASE model {self.name}...&#39;)
        device_map = self.device_map # if self.device_map else &#39;cpu&#39;
        if &#34;silo&#34; in self.name or &#34;balanced&#34; in self.name:
            from utils.transformers.model import OpenLMforCausalLM
            model = OpenLMforCausalLM.from_pretrained(
                self.name, **model_kwargs, device_map=self.device, cache_dir=self.cache_dir)
            # Extract the model from the model wrapper so we dont need to call model.model
        elif &#34;llama&#34; in self.name or &#34;alpaca&#34; in self.name:
            # TODO: This should be smth specified in config in case user has
            # llama is too big, gotta use device map
            model = transformers.AutoModelForCausalLM.from_pretrained(self.name, **model_kwargs, device_map=&#34;balanced_low_0&#34;, cache_dir=self.cache_dir)
            self.device = &#39;cuda:1&#39;
        elif &#34;stablelm&#34; in self.name.lower():  # models requiring custom code
            model = transformers.AutoModelForCausalLM.from_pretrained(
                self.name, **model_kwargs, trust_remote_code=True, device_map=device_map, cache_dir=self.cache_dir)
        elif &#34;olmo&#34; in self.name.lower():
            model = transformers.AutoModelForCausalLM.from_pretrained(
                self.name, **model_kwargs, trust_remote_code=True, cache_dir=self.cache_dir)
        else:
            model = transformers.AutoModelForCausalLM.from_pretrained(
                self.name, **model_kwargs, device_map=device_map, cache_dir=self.cache_dir)
    else:
        model = None

    optional_tok_kwargs = {}
    if &#34;facebook/opt-&#34; in self.name:
        print(&#34;Using non-fast tokenizer for OPT&#34;)
        optional_tok_kwargs[&#39;fast&#39;] = False
    if self.config.dataset_member in [&#39;pubmed&#39;] or self.config.dataset_nonmember in [&#39;pubmed&#39;]:
        optional_tok_kwargs[&#39;padding_side&#39;] = &#39;left&#39;
        self.pad_token = self.tokenizer.eos_token_id
    if &#34;silo&#34; in self.name or &#34;balanced&#34; in self.name:
        tokenizer = transformers.GPTNeoXTokenizerFast.from_pretrained(
            &#34;EleutherAI/gpt-neox-20b&#34;, **optional_tok_kwargs, cache_dir=self.cache_dir)
    elif &#34;datablations&#34; in self.name:
        tokenizer = transformers.AutoTokenizer.from_pretrained(
            &#34;gpt2&#34;, **optional_tok_kwargs, cache_dir=self.cache_dir)
    elif &#34;llama&#34; in self.name or &#34;alpaca&#34; in self.name:
        tokenizer = transformers.LlamaTokenizer.from_pretrained(
            self.name, **optional_tok_kwargs, cache_dir=self.cache_dir)
    elif &#34;pubmedgpt&#34; in self.name:
        tokenizer = transformers.AutoTokenizer.from_pretrained(
            &#34;stanford-crfm/BioMedLM&#34;, **optional_tok_kwargs, cache_dir=self.cache_dir)
    else:
        tokenizer = transformers.AutoTokenizer.from_pretrained(
            self.name, **optional_tok_kwargs, cache_dir=self.cache_dir,
            trust_remote_code=True if &#34;olmo&#34; in self.name.lower() else False)
    tokenizer.add_special_tokens({&#39;pad_token&#39;: &#39;[PAD]&#39;})

    return model, tokenizer</code></pre>
</details>
</dd>
<dt id="mimir.models.Model.load_model_properties"><code class="name flex">
<span>def <span class="ident">load_model_properties</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Load model properties, such as max length and stride.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_model_properties(self):
    &#34;&#34;&#34;
        Load model properties, such as max length and stride.
    &#34;&#34;&#34;
    # TODO: getting max_length of input could be more generic
    if &#34;silo&#34; in self.name or &#34;balanced&#34; in self.name:
        self.max_length = self.model.model.seq_len
    elif hasattr(self.model.config, &#39;max_position_embeddings&#39;):
        self.max_length = self.model.config.max_position_embeddings
    elif hasattr(self.model.config, &#39;n_positions&#39;):
        self.max_length = self.model.config.n_positions
    else:
        # Default window size
        self.max_length = 1024
    self.stride = self.max_length // 2</code></pre>
</details>
</dd>
<dt id="mimir.models.Model.to"><code class="name flex">
<span>def <span class="ident">to</span></span>(<span>self, device)</span>
</code></dt>
<dd>
<div class="desc"><p>Shift model to a particular device.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to(self, device):
    &#34;&#34;&#34;
        Shift model to a particular device.
    &#34;&#34;&#34;
    self.model.to(device, non_blocking=True)</code></pre>
</details>
</dd>
<dt id="mimir.models.Model.unload"><code class="name flex">
<span>def <span class="ident">unload</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Unload model from GPU</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def unload(self):
    &#34;&#34;&#34;
        Unload model from GPU
    &#34;&#34;&#34;
    start = time.time()
    try:
        self.model.cpu()
    except NameError:
        pass
    print(f&#39;DONE ({time.time() - start:.2f}s)&#39;)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="mimir.models.OpenAI_APIModel"><code class="flex name class">
<span>class <span class="ident">OpenAI_APIModel</span></span>
<span>(</span><span>config: <a title="mimir.config.ExperimentConfig" href="config.html#mimir.config.ExperimentConfig">ExperimentConfig</a>, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Wrapper for OpenAI API calls</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class OpenAI_APIModel(LanguageModel):
    &#34;&#34;&#34;
        Wrapper for OpenAI API calls
    &#34;&#34;&#34;
    def __init__(self, config: ExperimentConfig, **kwargs):
        super().__init__(config, **kwargs)
        self.model = None
        self.tokenizer = transformers.GPT2Tokenizer.from_pretrained(&#39;gpt2&#39;, cache_dir=self.cache_dir)
        self.API_TOKEN_COUNTER = 0
    
    @property
    def api_calls(self):
        &#34;&#34;&#34;
            Get the number of tokens used in API calls
        &#34;&#34;&#34;
        return self.API_TOKEN_COUNTER

    @torch.no_grad()
    def get_ll(self, text: str):
        &#34;&#34;&#34;
            Get the log likelihood of each text under the base_model
        &#34;&#34;&#34;
        openai_config = self.config.openai_config

        kwargs = {&#34;engine&#34;: openai_config.model, &#34;temperature&#34;: 0, &#34;max_tokens&#34;: 0, &#34;echo&#34;: True, &#34;logprobs&#34;: 0}
        r = openai.Completion.create(prompt=f&#34;&lt;|endoftext|&gt;{text}&#34;, **kwargs)
        result = r[&#39;choices&#39;][0]
        tokens, logprobs = result[&#34;logprobs&#34;][&#34;tokens&#34;][1:], result[&#34;logprobs&#34;][&#34;token_logprobs&#34;][1:]

        assert len(tokens) == len(logprobs), f&#34;Expected {len(tokens)} logprobs, got {len(logprobs)}&#34;

        return np.mean(logprobs)

    @torch.no_grad()
    def get_ref(self, text: str, ref_model: ReferenceModel):
        &#34;&#34;&#34;
            Get the  likelihood ratio of each text under the base_model -- MIA baseline
        &#34;&#34;&#34;
        raise NotImplementedError(&#34;OpenAI model not implemented for LIRA&#34;)
        openai_config = self.config.openai_config
        kwargs = {&#34;engine&#34;: openai_config.model, &#34;temperature&#34;: 0,
                    &#34;max_tokens&#34;: 0, &#34;echo&#34;: True, &#34;logprobs&#34;: 0}
        r = openai.Completion.create(prompt=f&#34;&lt;|endoftext|&gt;{text}&#34;, **kwargs)
        result = r[&#39;choices&#39;][0]
        tokens, logprobs = result[&#34;logprobs&#34;][&#34;tokens&#34;][1:], result[&#34;logprobs&#34;][&#34;token_logprobs&#34;][1:]

        assert len(tokens) == len(logprobs), f&#34;Expected {len(tokens)} logprobs, got {len(logprobs)}&#34;

        return np.mean(logprobs)

    def get_lls(self, texts: str):

        # use GPT2_TOKENIZER to get total number of tokens
        total_tokens = sum(len(self.tokenizer.encode(text)) for text in texts)
        self.API_TOKEN_COUNTER += total_tokens * 2  # multiply by two because OpenAI double-counts echo_prompt tokens

        pool = ThreadPool(self.config.batch_size)
        return pool.map(self.get_ll, texts)

    def _openai_sample(self, p: str):
        openai_config = self.config.openai_config
        if self.config.dataset_member != &#39;pubmed&#39;:  # keep Answer: prefix for pubmed
            p = drop_last_word(p)

        # sample from the openai model
        kwargs = { &#34;engine&#34;: openai_config.model, &#34;max_tokens&#34;: 200 }
        if self.config.do_top_p:
            kwargs[&#39;top_p&#39;] = self.config.top_p
    
        r = openai.Completion.create(prompt=f&#34;{p}&#34;, **kwargs)
        return p + r[&#39;choices&#39;][0].text


    def sample_from_model(self, texts: List[str], **kwargs):
        &#34;&#34;&#34;
            Sample from base_model using ****only**** the first 30 tokens in each example as context
        &#34;&#34;&#34;
        prompt_tokens = kwargs.get(&#39;prompt_tokens&#39;, 30)
        base_tokenizer = kwargs.get(&#39;base_tokenizer&#39;, None)
        if base_tokenizer is None:
            raise ValueError(&#34;Please provide base_tokenizer&#34;)

        # encode each text as a list of token ids
        if self.config.dataset_member == &#39;pubmed&#39;:
            texts = [t[:t.index(SEPARATOR)] for t in texts]
            all_encoded = base_tokenizer(texts, return_tensors=&#34;pt&#34;, padding=True).to(self.device)
        else:
            all_encoded = base_tokenizer(texts, return_tensors=&#34;pt&#34;, padding=True).to(self.device)
            all_encoded = {key: value[:, :prompt_tokens] for key, value in all_encoded.items()}

        # decode the prefixes back into text
        prefixes = base_tokenizer.batch_decode(all_encoded[&#39;input_ids&#39;], skip_special_tokens=True)
        pool = ThreadPool(self.config.batch_size)

        decoded = pool.map(self._openai_sample, prefixes)

        # count total number of tokens with GPT2_TOKENIZER
        total_tokens = sum(len(self.tokenizer.encode(x)) for x in decoded)
        self.API_TOKEN_COUNTER += total_tokens

        return decoded
    
    @torch.no_grad()
    def get_entropy(self, text: str):
        &#34;&#34;&#34;
            Get average entropy of each token in the text
        &#34;&#34;&#34;
        raise NotImplementedError(&#34;get_entropy not implemented for OpenAI models&#34;)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mimir.models.LanguageModel" href="#mimir.models.LanguageModel">LanguageModel</a></li>
<li><a title="mimir.models.Model" href="#mimir.models.Model">Model</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="mimir.models.OpenAI_APIModel.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="mimir.models.OpenAI_APIModel.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="mimir.models.OpenAI_APIModel.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="mimir.models.OpenAI_APIModel.api_calls"><code class="name">var <span class="ident">api_calls</span></code></dt>
<dd>
<div class="desc"><p>Get the number of tokens used in API calls</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def api_calls(self):
    &#34;&#34;&#34;
        Get the number of tokens used in API calls
    &#34;&#34;&#34;
    return self.API_TOKEN_COUNTER</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="mimir.models.OpenAI_APIModel.get_ll"><code class="name flex">
<span>def <span class="ident">get_ll</span></span>(<span>self, text: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the log likelihood of each text under the base_model</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.no_grad()
def get_ll(self, text: str):
    &#34;&#34;&#34;
        Get the log likelihood of each text under the base_model
    &#34;&#34;&#34;
    openai_config = self.config.openai_config

    kwargs = {&#34;engine&#34;: openai_config.model, &#34;temperature&#34;: 0, &#34;max_tokens&#34;: 0, &#34;echo&#34;: True, &#34;logprobs&#34;: 0}
    r = openai.Completion.create(prompt=f&#34;&lt;|endoftext|&gt;{text}&#34;, **kwargs)
    result = r[&#39;choices&#39;][0]
    tokens, logprobs = result[&#34;logprobs&#34;][&#34;tokens&#34;][1:], result[&#34;logprobs&#34;][&#34;token_logprobs&#34;][1:]

    assert len(tokens) == len(logprobs), f&#34;Expected {len(tokens)} logprobs, got {len(logprobs)}&#34;

    return np.mean(logprobs)</code></pre>
</details>
</dd>
<dt id="mimir.models.OpenAI_APIModel.get_lls"><code class="name flex">
<span>def <span class="ident">get_lls</span></span>(<span>self, texts: str)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_lls(self, texts: str):

    # use GPT2_TOKENIZER to get total number of tokens
    total_tokens = sum(len(self.tokenizer.encode(text)) for text in texts)
    self.API_TOKEN_COUNTER += total_tokens * 2  # multiply by two because OpenAI double-counts echo_prompt tokens

    pool = ThreadPool(self.config.batch_size)
    return pool.map(self.get_ll, texts)</code></pre>
</details>
</dd>
<dt id="mimir.models.OpenAI_APIModel.get_ref"><code class="name flex">
<span>def <span class="ident">get_ref</span></span>(<span>self, text: str, ref_model: <a title="mimir.models.ReferenceModel" href="#mimir.models.ReferenceModel">ReferenceModel</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the
likelihood ratio of each text under the base_model &ndash; MIA baseline</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.no_grad()
def get_ref(self, text: str, ref_model: ReferenceModel):
    &#34;&#34;&#34;
        Get the  likelihood ratio of each text under the base_model -- MIA baseline
    &#34;&#34;&#34;
    raise NotImplementedError(&#34;OpenAI model not implemented for LIRA&#34;)
    openai_config = self.config.openai_config
    kwargs = {&#34;engine&#34;: openai_config.model, &#34;temperature&#34;: 0,
                &#34;max_tokens&#34;: 0, &#34;echo&#34;: True, &#34;logprobs&#34;: 0}
    r = openai.Completion.create(prompt=f&#34;&lt;|endoftext|&gt;{text}&#34;, **kwargs)
    result = r[&#39;choices&#39;][0]
    tokens, logprobs = result[&#34;logprobs&#34;][&#34;tokens&#34;][1:], result[&#34;logprobs&#34;][&#34;token_logprobs&#34;][1:]

    assert len(tokens) == len(logprobs), f&#34;Expected {len(tokens)} logprobs, got {len(logprobs)}&#34;

    return np.mean(logprobs)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mimir.models.LanguageModel" href="#mimir.models.LanguageModel">LanguageModel</a></b></code>:
<ul class="hlist">
<li><code><a title="mimir.models.LanguageModel.forward" href="#mimir.models.Model.forward">forward</a></code></li>
<li><code><a title="mimir.models.LanguageModel.get_entropy" href="#mimir.models.LanguageModel.get_entropy">get_entropy</a></code></li>
<li><code><a title="mimir.models.LanguageModel.get_probabilities" href="#mimir.models.Model.get_probabilities">get_probabilities</a></code></li>
<li><code><a title="mimir.models.LanguageModel.get_rank" href="#mimir.models.LanguageModel.get_rank">get_rank</a></code></li>
<li><code><a title="mimir.models.LanguageModel.load" href="#mimir.models.Model.load">load</a></code></li>
<li><code><a title="mimir.models.LanguageModel.load_base_model_and_tokenizer" href="#mimir.models.Model.load_base_model_and_tokenizer">load_base_model_and_tokenizer</a></code></li>
<li><code><a title="mimir.models.LanguageModel.load_model_properties" href="#mimir.models.Model.load_model_properties">load_model_properties</a></code></li>
<li><code><a title="mimir.models.LanguageModel.sample_from_model" href="#mimir.models.LanguageModel.sample_from_model">sample_from_model</a></code></li>
<li><code><a title="mimir.models.LanguageModel.to" href="#mimir.models.Model.to">to</a></code></li>
<li><code><a title="mimir.models.LanguageModel.unload" href="#mimir.models.Model.unload">unload</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mimir.models.QuantileReferenceModel"><code class="flex name class">
<span>class <span class="ident">QuantileReferenceModel</span></span>
<span>(</span><span>config: <a title="mimir.config.ExperimentConfig" href="config.html#mimir.config.ExperimentConfig">ExperimentConfig</a>, name: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Wrapper for referenc model, specifically used for quantile regression</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class QuantileReferenceModel(Model):
    &#34;&#34;&#34;
        Wrapper for referenc model, specifically used for quantile regression
    &#34;&#34;&#34;
    def __init__(self, config: ExperimentConfig, name: str):
        super().__init__(config)
        self.device = self.config.env_config.device_aux
        self.name = name
        self.tokenizer = AutoTokenizer.from_pretrained(
            name, use_fast=False)
        self.model = AutoModelForSequenceClassification.from_pretrained(
            name,
            num_labels=2,
            max_position_embeddings=1024)
        # Modify model&#39;s last linear layer to have only 1 output
        self.model.classifier.linear_out = nn.Linear(self.model.classifier.linear_out.in_features, 1)
        self.load_model_properties()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mimir.models.Model" href="#mimir.models.Model">Model</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="mimir.models.QuantileReferenceModel.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="mimir.models.QuantileReferenceModel.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="mimir.models.QuantileReferenceModel.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mimir.models.Model" href="#mimir.models.Model">Model</a></b></code>:
<ul class="hlist">
<li><code><a title="mimir.models.Model.forward" href="#mimir.models.Model.forward">forward</a></code></li>
<li><code><a title="mimir.models.Model.get_ll" href="#mimir.models.Model.get_ll">get_ll</a></code></li>
<li><code><a title="mimir.models.Model.get_probabilities" href="#mimir.models.Model.get_probabilities">get_probabilities</a></code></li>
<li><code><a title="mimir.models.Model.load" href="#mimir.models.Model.load">load</a></code></li>
<li><code><a title="mimir.models.Model.load_base_model_and_tokenizer" href="#mimir.models.Model.load_base_model_and_tokenizer">load_base_model_and_tokenizer</a></code></li>
<li><code><a title="mimir.models.Model.load_model_properties" href="#mimir.models.Model.load_model_properties">load_model_properties</a></code></li>
<li><code><a title="mimir.models.Model.to" href="#mimir.models.Model.to">to</a></code></li>
<li><code><a title="mimir.models.Model.unload" href="#mimir.models.Model.unload">unload</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="mimir.models.ReferenceModel"><code class="flex name class">
<span>class <span class="ident">ReferenceModel</span></span>
<span>(</span><span>config: <a title="mimir.config.ExperimentConfig" href="config.html#mimir.config.ExperimentConfig">ExperimentConfig</a>, name: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Wrapper for reference model</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ReferenceModel(Model):
    &#34;&#34;&#34;
        Wrapper for reference model
    &#34;&#34;&#34;
    def __init__(self, config: ExperimentConfig, name: str):
        super().__init__(config)
        self.device = self.config.env_config.device_aux
        self.name = name
        base_model_kwargs = {&#39;revision&#39;: &#39;main&#39;}
        if &#39;gpt-j&#39; in self.name or &#39;neox&#39; in self.name or &#39;llama&#39; in self.name or &#39;alpaca&#39; in self.name:
            base_model_kwargs.update(dict(torch_dtype=torch.float16))
        if &#39;gpt-j&#39; in self.name:
            base_model_kwargs.update(dict(revision=&#39;float16&#39;))
        if &#39;:&#39; in self.name:
            print(&#34;Applying ref model revision&#34;)
            # Allow them to provide revisions as part of model name, then parse accordingly
            split = self.name.split(&#39;:&#39;)
            self.name = split[0]
            base_model_kwargs.update(dict(revision=split[-1]))
        self.model, self.tokenizer = self.load_base_model_and_tokenizer(
            model_kwargs=base_model_kwargs)
        self.load_model_properties()

    def load(self):
        &#34;&#34;&#34;
        Load reference model noto GPU(s)
        &#34;&#34;&#34;
        if &#34;llama&#34; not in self.name and &#34;alpaca&#34; not in self.name:
            super().load()

    def unload(self):
        &#34;&#34;&#34;
        Unload reference model from GPU(s)
        &#34;&#34;&#34;
        if &#34;llama&#34; not in self.name and &#34;alpaca&#34; not in self.name:
            super().unload()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="mimir.models.Model" href="#mimir.models.Model">Model</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="mimir.models.ReferenceModel.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="mimir.models.ReferenceModel.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="mimir.models.ReferenceModel.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="mimir.models.ReferenceModel.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Load reference model noto GPU(s)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load(self):
    &#34;&#34;&#34;
    Load reference model noto GPU(s)
    &#34;&#34;&#34;
    if &#34;llama&#34; not in self.name and &#34;alpaca&#34; not in self.name:
        super().load()</code></pre>
</details>
</dd>
<dt id="mimir.models.ReferenceModel.unload"><code class="name flex">
<span>def <span class="ident">unload</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Unload reference model from GPU(s)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def unload(self):
    &#34;&#34;&#34;
    Unload reference model from GPU(s)
    &#34;&#34;&#34;
    if &#34;llama&#34; not in self.name and &#34;alpaca&#34; not in self.name:
        super().unload()</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="mimir.models.Model" href="#mimir.models.Model">Model</a></b></code>:
<ul class="hlist">
<li><code><a title="mimir.models.Model.forward" href="#mimir.models.Model.forward">forward</a></code></li>
<li><code><a title="mimir.models.Model.get_ll" href="#mimir.models.Model.get_ll">get_ll</a></code></li>
<li><code><a title="mimir.models.Model.get_probabilities" href="#mimir.models.Model.get_probabilities">get_probabilities</a></code></li>
<li><code><a title="mimir.models.Model.load_base_model_and_tokenizer" href="#mimir.models.Model.load_base_model_and_tokenizer">load_base_model_and_tokenizer</a></code></li>
<li><code><a title="mimir.models.Model.load_model_properties" href="#mimir.models.Model.load_model_properties">load_model_properties</a></code></li>
<li><code><a title="mimir.models.Model.to" href="#mimir.models.Model.to">to</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<header>
<a class="homelink" rel="home" title="MIMIR Home" href="https://iamgroot42.github.io/mimir/">
<img src="https://raw.githubusercontent.com/iamgroot42/mimir/8ed6886fb6df7a72f2f0f398688f48b68c5f48b0/assets/logo.png" alt="MIMIR">
</a>
</header>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="mimir" href="index.html">mimir</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="mimir.models.LanguageModel" href="#mimir.models.LanguageModel">LanguageModel</a></code></h4>
<ul class="two-column">
<li><code><a title="mimir.models.LanguageModel.call_super_init" href="#mimir.models.LanguageModel.call_super_init">call_super_init</a></code></li>
<li><code><a title="mimir.models.LanguageModel.dump_patches" href="#mimir.models.LanguageModel.dump_patches">dump_patches</a></code></li>
<li><code><a title="mimir.models.LanguageModel.get_entropy" href="#mimir.models.LanguageModel.get_entropy">get_entropy</a></code></li>
<li><code><a title="mimir.models.LanguageModel.get_lls" href="#mimir.models.LanguageModel.get_lls">get_lls</a></code></li>
<li><code><a title="mimir.models.LanguageModel.get_max_norm" href="#mimir.models.LanguageModel.get_max_norm">get_max_norm</a></code></li>
<li><code><a title="mimir.models.LanguageModel.get_rank" href="#mimir.models.LanguageModel.get_rank">get_rank</a></code></li>
<li><code><a title="mimir.models.LanguageModel.get_ref" href="#mimir.models.LanguageModel.get_ref">get_ref</a></code></li>
<li><code><a title="mimir.models.LanguageModel.sample_from_model" href="#mimir.models.LanguageModel.sample_from_model">sample_from_model</a></code></li>
<li><code><a title="mimir.models.LanguageModel.training" href="#mimir.models.LanguageModel.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mimir.models.Model" href="#mimir.models.Model">Model</a></code></h4>
<ul class="">
<li><code><a title="mimir.models.Model.call_super_init" href="#mimir.models.Model.call_super_init">call_super_init</a></code></li>
<li><code><a title="mimir.models.Model.dump_patches" href="#mimir.models.Model.dump_patches">dump_patches</a></code></li>
<li><code><a title="mimir.models.Model.forward" href="#mimir.models.Model.forward">forward</a></code></li>
<li><code><a title="mimir.models.Model.get_ll" href="#mimir.models.Model.get_ll">get_ll</a></code></li>
<li><code><a title="mimir.models.Model.get_probabilities" href="#mimir.models.Model.get_probabilities">get_probabilities</a></code></li>
<li><code><a title="mimir.models.Model.load" href="#mimir.models.Model.load">load</a></code></li>
<li><code><a title="mimir.models.Model.load_base_model_and_tokenizer" href="#mimir.models.Model.load_base_model_and_tokenizer">load_base_model_and_tokenizer</a></code></li>
<li><code><a title="mimir.models.Model.load_model_properties" href="#mimir.models.Model.load_model_properties">load_model_properties</a></code></li>
<li><code><a title="mimir.models.Model.to" href="#mimir.models.Model.to">to</a></code></li>
<li><code><a title="mimir.models.Model.training" href="#mimir.models.Model.training">training</a></code></li>
<li><code><a title="mimir.models.Model.unload" href="#mimir.models.Model.unload">unload</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mimir.models.OpenAI_APIModel" href="#mimir.models.OpenAI_APIModel">OpenAI_APIModel</a></code></h4>
<ul class="two-column">
<li><code><a title="mimir.models.OpenAI_APIModel.api_calls" href="#mimir.models.OpenAI_APIModel.api_calls">api_calls</a></code></li>
<li><code><a title="mimir.models.OpenAI_APIModel.call_super_init" href="#mimir.models.OpenAI_APIModel.call_super_init">call_super_init</a></code></li>
<li><code><a title="mimir.models.OpenAI_APIModel.dump_patches" href="#mimir.models.OpenAI_APIModel.dump_patches">dump_patches</a></code></li>
<li><code><a title="mimir.models.OpenAI_APIModel.get_ll" href="#mimir.models.OpenAI_APIModel.get_ll">get_ll</a></code></li>
<li><code><a title="mimir.models.OpenAI_APIModel.get_lls" href="#mimir.models.OpenAI_APIModel.get_lls">get_lls</a></code></li>
<li><code><a title="mimir.models.OpenAI_APIModel.get_ref" href="#mimir.models.OpenAI_APIModel.get_ref">get_ref</a></code></li>
<li><code><a title="mimir.models.OpenAI_APIModel.training" href="#mimir.models.OpenAI_APIModel.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mimir.models.QuantileReferenceModel" href="#mimir.models.QuantileReferenceModel">QuantileReferenceModel</a></code></h4>
<ul class="">
<li><code><a title="mimir.models.QuantileReferenceModel.call_super_init" href="#mimir.models.QuantileReferenceModel.call_super_init">call_super_init</a></code></li>
<li><code><a title="mimir.models.QuantileReferenceModel.dump_patches" href="#mimir.models.QuantileReferenceModel.dump_patches">dump_patches</a></code></li>
<li><code><a title="mimir.models.QuantileReferenceModel.training" href="#mimir.models.QuantileReferenceModel.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="mimir.models.ReferenceModel" href="#mimir.models.ReferenceModel">ReferenceModel</a></code></h4>
<ul class="">
<li><code><a title="mimir.models.ReferenceModel.call_super_init" href="#mimir.models.ReferenceModel.call_super_init">call_super_init</a></code></li>
<li><code><a title="mimir.models.ReferenceModel.dump_patches" href="#mimir.models.ReferenceModel.dump_patches">dump_patches</a></code></li>
<li><code><a title="mimir.models.ReferenceModel.load" href="#mimir.models.ReferenceModel.load">load</a></code></li>
<li><code><a title="mimir.models.ReferenceModel.training" href="#mimir.models.ReferenceModel.training">training</a></code></li>
<li><code><a title="mimir.models.ReferenceModel.unload" href="#mimir.models.ReferenceModel.unload">unload</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>